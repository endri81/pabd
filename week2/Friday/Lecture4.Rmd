---
title: "Deep Learning in R"
subtitle: "From Forward Propagation to Fine-Tuning"
author: |
  | Prof. Asc. Endri Raco, Ph.D.
  | Polytechnic University of Tirana
date: "November 2025"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "default"
    fonttheme: "professionalfonts"
    slide_level: 2
header-includes:
  - \usepackage{tikz}
  - \usepackage{amsmath}
  - \definecolor{upcblue}{RGB}{0,51,102}
  - \setbeamercolor{structure}{fg=upcblue}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(keras)
library(tidyverse)
set.seed(42)
```

# Introduction

## What You Will Master Today

**Conceptual Understanding**

- Neural network architecture
- Forward and backward propagation  
- Loss functions and optimization
- Model validation strategies

**Implementation Skills**

- Build networks in R with keras
- Train and validate models
- Tune hyperparameters
- Diagnose training problems

**Critical:** Theory meets practice throughout

---

## Course Structure

First two chapters focus on conceptual knowledge

- Debug and tune deep learning models on conventional prediction problems
- Lay the foundation for progressing towards modern applications

This will pay off in the third and fourth chapters

---

## Why Deep Learning?

**The Power of Interactions**

Neural networks account for interactions really well

Deep learning uses especially powerful neural networks

Applications:

- Text
- Images  
- Videos
- Audio
- Structured data

---

## Traditional ML vs Neural Networks

**Traditional Models:**

- Manually specify interactions
- Test hundreds of feature combinations
- Requires domain expertise

**Neural Networks:**

- Automatically discover patterns
- Learn hierarchical representations
- No manual feature engineering

Example: Customer churn = usage decline + support contact + competitor promo

---

# Chapter 1: Forward Propagation

## What is Forward Propagation?

**Definition:** The process of computing predictions from inputs through the network

\begin{center}
\begin{tikzpicture}[scale=0.8]
\node[circle,draw,fill=green!30,minimum size=1cm] (i1) at (0,2) {2};
\node[circle,draw,fill=green!30,minimum size=1cm] (i2) at (0,0) {3};
\node[circle,draw,fill=blue!30,minimum size=1cm] (h1) at (3,2.5) {};
\node[circle,draw,fill=blue!30,minimum size=1cm] (h2) at (3,0.5) {};
\node[circle,draw,fill=blue!30,minimum size=1cm] (o) at (6,1.5) {};

\draw[->,thick] (i1) -- node[above] {$w_1$} (h1);
\draw[->,thick] (i1) -- (h2);
\draw[->,thick] (i2) -- (h1);
\draw[->,thick] (i2) -- node[below] {$w_2$} (h2);
\draw[->,thick] (h1) -- (o);
\draw[->,thick] (h2) -- (o);

\node at (0,-1) {Input};
\node at (3,-1) {Hidden};
\node at (6,-1) {Output};
\end{tikzpicture}
\end{center}

Information flows left to right

---

## A Simple Example

\begin{center}
\begin{tikzpicture}[scale=0.9]
\node[circle,draw,fill=green!30,minimum size=1cm] (i1) at (0,2) {2};
\node[circle,draw,fill=green!30,minimum size=1cm] (i2) at (0,0) {3};
\node[circle,draw,fill=blue!30,minimum size=1cm] (h1) at (3,2.5) {5};
\node[circle,draw,fill=blue!30,minimum size=1cm] (h2) at (3,0.5) {1};
\node[circle,draw,fill=blue!30,minimum size=1cm] (o) at (6,1.5) {9};

\draw[->,thick] (i1) -- node[above,sloped] {1} (h1);
\draw[->,thick] (i1) -- node[above,sloped] {-1} (h2);
\draw[->,thick] (i2) -- node[below,sloped] {1} (h1);
\draw[->,thick] (i2) -- node[below,sloped] {1} (h2);
\draw[->,thick] (h1) -- node[above,sloped] {2} (o);
\draw[->,thick] (h2) -- node[below,sloped] {-1} (o);

\node at (0,-1) {Input};
\node at (3,-1) {Hidden Layer};
\node at (6,-1) {Output};
\end{tikzpicture}
\end{center}

Let's compute this step by step!

---

## Step 1: Compute Hidden Node 0

**Input values:** 2, 3

**Weights to node 0:** 1, 1

**Computation:**

$$\text{node}_0 = (2 \times 1) + (3 \times 1) = 5$$

Simple weighted sum!

---

## Step 2: Compute Hidden Node 1

**Input values:** 2, 3

**Weights to node 1:** -1, 1

**Computation:**

$$\text{node}_1 = (2 \times -1) + (3 \times 1) = 1$$

Each node does the same operation

---

## Step 3: Compute Output

**Hidden layer values:** 5, 1

**Weights to output:** 2, -1

**Computation:**

$$\text{output} = (5 \times 2) + (1 \times -1) = 9$$

Final prediction!

---

## Forward Propagation in R - Setup
```{r}
# Input data
input_data <- c(2, 3)

# Define weight matrices
weights <- list(
  node_0 = c(1, 1),
  node_1 = c(-1, 1),
  output = c(2, -1)
)
```

---

## Forward Propagation in R - Hidden Layer
```{r}
# Compute hidden layer values
node_0_value <- sum(input_data * weights$node_0)
node_1_value <- sum(input_data * weights$node_1)

# Show results
hidden_layer <- c(node_0_value, node_1_value)
print(hidden_layer)
```

---

## Forward Propagation in R - Output
```{r}
# Compute output
output <- sum(hidden_layer * weights$output)
print(output)
```

That's forward propagation!

---

## Why Do We Need Activation Functions?

**Without activation:** Network reduces to linear model

$$f(f(x)) = \text{still linear}$$

**With activation:** Network can represent complex patterns

$$f(\text{ReLU}(x)) = \text{nonlinear}$$

Activation functions introduce nonlinearity

---

## Common Activation Functions

**ReLU (Rectified Linear Unit):**

$$\text{ReLU}(x) = \max(0, x)$$

**Tanh (Hyperbolic Tangent):**

$$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**Sigmoid:**

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

---

## ReLU Activation

\begin{center}
\begin{tikzpicture}[scale=0.7]
\draw[->] (-3,0) -- (3,0) node[right] {$x$};
\draw[->] (0,-1) -- (0,3) node[above] {$\text{ReLU}(x)$};
\draw[thick,blue] (-3,0) -- (0,0);
\draw[thick,blue] (0,0) -- (2.5,2.5);
\node at (0,-1.5) {Output = max(0, input)};
\end{tikzpicture}
\end{center}

**Properties:**

- Simple and fast
- Avoids vanishing gradients
- Most popular choice

---

## Tanh Activation

\begin{center}
\begin{tikzpicture}[scale=0.7]
\draw[->] (-3,0) -- (3,0) node[right] {$x$};
\draw[->] (0,-2) -- (0,2) node[above] {$\text{tanh}(x)$};
\draw[thick,blue,domain=-3:3,samples=100] plot (\x,{tanh(\x)});
\draw[dashed] (-3,-1) -- (3,-1);
\draw[dashed] (-3,1) -- (3,1);
\node at (0,-2.5) {Output range: [-1, 1]};
\end{tikzpicture}
\end{center}

**Properties:**

- Zero-centered
- Smooth gradient

---

## Forward Propagation with Activation
```{r}
# Input and weights
input_data <- c(-1, 2)
weights <- list(
  node_0 = c(3, 3),
  node_1 = c(1, 5),
  output = c(2, -1)
)
```

---

## Compute Hidden Layer with Tanh
```{r}
# Compute with activation
node_0_input <- sum(input_data * weights$node_0)
node_0_output <- tanh(node_0_input)

node_1_input <- sum(input_data * weights$node_1)
node_1_output <- tanh(node_1_input)

hidden_outputs <- c(node_0_output, node_1_output)
print(hidden_outputs)
```

---

## Final Output
```{r}
# Compute final output
output <- sum(hidden_outputs * weights$output)
print(output)
```

Compare this to output without activation!

---

## Multiple Observations

**Reality:** We predict for many data points at once

**Solution:** Use matrices instead of vectors

- Each row = one observation
- Matrix multiplication handles all at once
- Much faster than loops

---

## Matrix Representation
```{r}
# Multiple observations (3 customers, 2 features)
input_data <- matrix(c(2, 3,
                       1, 4,
                       3, 2), 
                     nrow = 3, byrow = TRUE)

print(input_data)
```

---

## Matrix Forward Propagation - Setup
```{r}
# Weight matrix for hidden layer (2 inputs, 2 hidden)
W_hidden <- matrix(c(1, 1,
                     -1, 1), 
                   nrow = 2, byrow = FALSE)

# Weight vector for output (2 hidden, 1 output)
W_output <- c(2, -1)
```

---

## Matrix Forward Propagation - Compute
```{r}
# Hidden layer (with ReLU)
hidden <- pmax(0, input_data %*% W_hidden)
print(hidden)

# Output layer
output <- hidden %*% W_output
print(output)
```

---

## Deep Networks

**Definition:** Networks with multiple hidden layers

\begin{center}
\begin{tikzpicture}[scale=0.6]
\foreach \y in {1,2,3} {
  \node[circle,draw,fill=green!30] at (0,\y) {};
}
\foreach \y in {0.5,1.5,2.5,3.5} {
  \node[circle,draw,fill=blue!30] at (2,\y) {};
}
\foreach \y in {0.5,1.5,2.5,3.5} {
  \node[circle,draw,fill=blue!30] at (4,\y) {};
}
\foreach \y in {1.5,2.5} {
  \node[circle,draw,fill=blue!30] at (6,\y) {};
}
\node[circle,draw,fill=red!30] at (8,2) {};

\node at (0,-0.5) {Input};
\node at (2,-0.5) {Hidden 1};
\node at (4,-0.5) {Hidden 2};
\node at (6,-0.5) {Hidden 3};
\node at (8,-0.5) {Output};
\end{tikzpicture}
\end{center}

More layers = more complex patterns

---

## Why Go Deep?

**Hierarchical Learning:**

- Layer 1: Simple patterns
- Layer 2: Combinations
- Layer 3: Complex concepts
- Output: Final decision

Example: Image recognition

- Edges → Shapes → Objects → Classification

---

## Building a Deep Network in R
```{r, eval=FALSE}
# Load keras
library(keras)

# Define network architecture
model <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = 'relu', 
              input_shape = 2) %>%
  layer_dense(units = 10, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'relu') %>%
  layer_dense(units = 1)
```

Three hidden layers of 10 nodes each

---

## Model Summary
```{r, eval=FALSE}
summary(model)
```
```
_____________________________________________
Layer (type)        Output Shape     Param #
=============================================
dense_1 (Dense)     (None, 10)       30
dense_2 (Dense)     (None, 10)       110
dense_3 (Dense)     (None, 10)       110
dense_4 (Dense)     (None, 1)        11
=============================================
Total params: 261
Trainable params: 261
```

---

## Counting Parameters

**First hidden layer:** 

- Inputs: 2
- Nodes: 10
- Parameters: (2 × 10) + 10 = 30

**Second hidden layer:**

- Inputs: 10  
- Nodes: 10
- Parameters: (10 × 10) + 10 = 110

Weights + biases for each node

---

## Network Representation Power

**Universal Approximation Theorem:**

A neural network with:

- One hidden layer
- Sufficient nodes  
- Nonlinear activation

Can approximate any continuous function

But deeper networks learn more efficiently!

---

## Practical Considerations

**Too shallow:**

- Cannot capture complex patterns
- Underfitting

**Too deep:**

- May overfit
- Harder to train
- Vanishing gradients

Start simple, add depth as needed

---

## Forward Propagation: Complete Example
```{r, eval=FALSE}
# Generate sample data
X <- matrix(rnorm(1000 * 3), ncol = 3)
y <- rowSums(X^2) + rnorm(1000, sd = 0.1)

# Build model
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', 
              input_shape = 3) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1)
```

---

## Make Predictions (Forward Pass)
```{r, eval=FALSE}
# Random initial predictions
predictions <- model %>% predict(X[1:5, ])
print(predictions)
```
```
     [,1]
[1,] -0.234
[2,]  0.891
[3,] -0.567
[4,]  0.123
[5,] -0.345
```

These are bad - network not trained yet!

---

## Key Concepts Summary

**Forward propagation:**

- Multiply inputs by weights
- Add bias terms
- Apply activation function
- Repeat for each layer

**Matrix operations:**

- Handle multiple observations efficiently
- Standard linear algebra

**Deep networks:**

- Stack multiple layers
- Each adds representation power

---

# Chapter 2: Optimization & Backpropagation

## The Learning Problem

**Question:** How do we find good weights?

**Answer:** Optimization!

1. Define a loss function (how wrong are we?)
2. Compute gradients (which direction improves?)
3. Update weights (take a step)
4. Repeat

---

## Loss Functions

**Regression:** Mean Squared Error

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**Classification:** Categorical Crossentropy

$$\text{CCE} = -\sum_{i=1}^{n}\sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})$$

Lower loss = better predictions

---

## Mean Squared Error Example
```{r}
# True values
y_true <- c(2.5, 3.0, 4.5)

# Predicted values
y_pred <- c(2.3, 3.2, 4.1)

# Compute MSE
mse <- mean((y_true - y_pred)^2)
print(mse)
```

---

## Why Minimize Loss?

\begin{center}
\begin{tikzpicture}[scale=0.8]
\draw[->] (0,0) -- (5,0) node[right] {Weight};
\draw[->] (0,0) -- (0,4) node[above] {Loss};
\draw[thick,blue,domain=0:4.5] plot (\x,{0.3*(\x-2.5)^2 + 0.5});
\node[circle,fill=red,inner sep=2pt] at (2.5,0.5) {};
\node at (2.5,-0.5) {Optimal};
\end{tikzpicture}
\end{center}

Find the bottom of the bowl!

---

## Gradient Descent

**Idea:** Move in direction of steepest descent

$$w_{\text{new}} = w_{\text{old}} - \eta \frac{\partial L}{\partial w}$$

where:

- $\eta$ = learning rate
- $\frac{\partial L}{\partial w}$ = gradient

---

## Learning Rate Matters

\begin{center}
\begin{tikzpicture}[scale=0.7]
\draw[->] (0,0) -- (6,0) node[right] {Weight};
\draw[->] (0,0) -- (0,4) node[above] {Loss};
\draw[thick,blue,domain=0:5.5] plot (\x,{0.3*(\x-3)^2 + 0.5});

% Too large
\draw[->,red,very thick] (1,2.2) -- (5,3.5);
\node[red] at (3,4) {Too large: Overshoots};

% Just right
\draw[->,green,thick] (1,2.2) -- (1.5,1.7);
\draw[->,green,thick] (1.5,1.7) -- (2,1.3);
\node[green] at (1,1) {Just right};
\end{tikzpicture}
\end{center}

---

## Backpropagation: The Idea

**Problem:** Network has thousands of weights

**Solution:** Chain rule of calculus!

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial h} \cdot \frac{\partial h}{\partial w_1}$$

Work backwards through the network

---

## Backpropagation Visualization

\begin{center}
\begin{tikzpicture}[scale=0.8]
\node[circle,draw] (i) at (0,0) {$x$};
\node[circle,draw] (h) at (2,0) {$h$};
\node[circle,draw] (o) at (4,0) {$\hat{y}$};
\node[circle,draw] (l) at (6,0) {$L$};

\draw[->,thick] (i) -- node[above] {$w_1$} (h);
\draw[->,thick] (h) -- node[above] {$w_2$} (o);
\draw[->,thick] (o) -- (l);

\draw[->,red,thick,dashed] (l) -- node[below] {$\frac{\partial L}{\partial \hat{y}}$} (o);
\draw[->,red,thick,dashed] (o) -- node[below] {$\frac{\partial \hat{y}}{\partial h}$} (h);
\draw[->,red,thick,dashed] (h) -- node[below] {$\frac{\partial h}{\partial w_1}$} (i);
\end{tikzpicture}
\end{center}

Forward: compute predictions → Backward: compute gradients

---

## What is Fitting a Model?

**Fitting = Training = Learning**

Applying backpropagation and gradient descent with your data to update the weights

Process:

1. Forward pass: compute predictions
2. Compute loss
3. Backward pass: compute gradients  
4. Update weights
5. Repeat

---

## Compiling a Model in R
```{r, eval=FALSE}
# Build model
model <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = 'relu', 
              input_shape = 10) %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dense(units = 1)

# Compile: specify optimizer and loss
model %>% compile(
  optimizer = 'adam',
  loss = 'mean_squared_error'
)
```

---

## Why Compile?

**Specify the optimizer:**

- Many options and mathematically complex
- "Adam" is usually a good choice
- Adaptive learning rates

**Specify loss function:**

- `mean_squared_error` for regression
- `categorical_crossentropy` for classification

Compilation prepares model for training

---

I'll continue with the next batch. Should I proceed with slides 61-120 covering the rest of Chapter 2 and beginning Chapter 3?