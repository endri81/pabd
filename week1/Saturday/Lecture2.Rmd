---
title: |
  | **The Basetable Timeline**
  | Intermediate Predictive Analytics
subtitle: "Constructing Temporal Structures for Predictive Modeling"
author: "Prof. Asc.Endri Raco, Ph.D."
institute: |
  | Department of Mathematical Engineering
  | Polytechnic University of Tirana
date: "November 2025"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "default"
    fonttheme: "professionalfonts"
    slide_level: 2
    toc: false
    keep_tex: false
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage{tikz}
  - \usepackage{xcolor}
  - \definecolor{datablue}{RGB}{0,102,204}
  - \definecolor{datagreen}{RGB}{0,153,76}
  - \definecolor{datared}{RGB}{204,0,0}
  - \definecolor{dataorange}{RGB}{255,140,0}
  - \definecolor{datapurple}{RGB}{128,0,128}
  - \definecolor{datacyan}{RGB}{0,191,255}
  - \definecolor{datalime}{RGB}{50,205,50}
  - \setbeamercolor{structure}{fg=datablue}
  - \setbeamertemplate{navigation symbols}{}
  - \setbeamertemplate{footline}[frame number]
  - \setbeamertemplate{frametitle}{\vspace{0.5em}\insertframetitle}
  - \newcommand{\pbar}{\overline{p}}
  - \usepackage{pifont}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  fig.width = 6,
  fig.height = 4,
  out.width = '80%'
)

# Load required libraries
library(tidyverse)
library(lubridate)
library(knitr)
library(kableExtra)
```

# Introduction

## The Predictive Modeling Process

\begin{block}{Foundations of Predictive Analytics I}
\begin{itemize}
  \item Build predictive models
  \item Evaluate predictive models
  \item Present predictive models to business stakeholders
\end{itemize}
\end{block}

\begin{block}{Foundations of Predictive Analytics II}
\begin{itemize}
  \item \textcolor{datagreen}{\textbf{Construct the basetable}}
\end{itemize}
\end{block}

## Learning Objectives

\begin{block}{By the end of this lecture, you will be able to:}
\begin{enumerate}
  \item Define and construct a basetable for predictive modeling
  \item Understand the temporal structure of prediction problems
  \item Implement timeline-compliant data partitioning
  \item Define population eligibility criteria
  \item Create binary and continuous target variables
  \item Apply set operations for population filtering
\end{enumerate}
\end{block}

# The Basetable

## What is a Basetable? (1/4)

\begin{center}
\begin{tikzpicture}[scale=0.8]
  % Draw the main rectangle
  \draw[line width=2pt, black] (0,0) rectangle (10,3);
  
  % Draw vertical separator
  \draw[line width=2pt, black] (8,0) -- (8,3);
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{block}{Definition}
A basetable is a \textbf{structured data matrix} where:
\begin{itemize}
  \item Each \textcolor{datablue}{\textbf{row}} represents an observation unit (customer, donor, patient)
  \item Each \textcolor{dataorange}{\textbf{column}} represents a variable (predictor or target)
\end{itemize}
\end{block}

## What is a Basetable? (2/4)

\begin{center}
\begin{tikzpicture}[scale=0.8]
  % Draw the main rectangle
  \draw[line width=2pt, black] (0,0) rectangle (10,3);
  
  % Draw vertical separator
  \draw[line width=2pt, black] (8,0) -- (8,3);
  
  % Add population icons on left
  \node at (-1, 2.3) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 2.3) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 1.5) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 1.5) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 0.7) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 0.7) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  % Add label
  \node[left] at (-1.5, 1.5) {\textbf{Population}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{block}{Population}
The set of \textcolor{datablue}{\textbf{observation units}} eligible for analysis.
\end{block}

## What is a Basetable? (3/4)

\begin{center}
\begin{tikzpicture}[scale=0.7]
  % Draw the main rectangle
  \draw[line width=2pt, black] (0,0) rectangle (10,3);
  
  % Draw vertical separator
  \draw[line width=2pt, black] (8,0) -- (8,3);
  
  % Add population icons on left
  \node at (-1, 2.3) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 2.3) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 1.5) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 1.5) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 0.7) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 0.7) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  % Add label
  \node[left] at (-1.5, 1.5) {\textbf{Population}};
  
  % Add column headers
  \node at (2, 3.5) {\small \textbf{Age}};
  \node at (4, 3.5) {\small \textbf{Gender}};
  \node at (6, 3.5) {\small \textbf{Previous gifts}};
  
  % Add data
  \node at (2, 2.3) {25};
  \node at (4, 2.3) {F};
  \node at (6, 2.3) {12};
  
  \node at (2, 1.5) {60};
  \node at (4, 1.5) {M};
  \node at (6, 1.5) {5};
  
  \node at (2, 0.7) {45};
  \node at (4, 0.7) {F};
  \node at (6, 0.7) {9};
  
  % Add header label
  \node at (4, 4.2) {\textcolor{datagreen}{\textbf{Candidate predictors}}};
\end{tikzpicture}
\end{center}

\begin{block}{Candidate Predictors}
Historical features calculated from data available \textcolor{datagreen}{\textbf{before}} the observation point.
\end{block}

## What is a Basetable? (4/4)

\begin{center}
\begin{tikzpicture}[scale=0.65]
  % Draw the main rectangle
  \draw[line width=2pt, black] (0,0) rectangle (10,3);
  
  % Draw vertical separator
  \draw[line width=2pt, black] (8,0) -- (8,3);
  
  % Add population icons on left
  \node at (-1, 2.3) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 2.3) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 1.5) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 1.5) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  \node at (-1, 0.7) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 0.7) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  
  % Add label
  \node[left] at (-1.5, 1.5) {\textbf{Population}};
  
  % Add column headers
  \node at (2, 3.5) {\small \textbf{Age}};
  \node at (4, 3.5) {\small \textbf{Gender}};
  \node at (6, 3.5) {\small \textbf{Previous gifts}};
  \node at (9, 3.5) {\small \textbf{Donate}};
  
  % Add data
  \node at (2, 2.3) {25};
  \node at (4, 2.3) {F};
  \node at (6, 2.3) {12};
  \node at (9, 2.3) {0};
  
  \node at (2, 1.5) {60};
  \node at (4, 1.5) {M};
  \node at (6, 1.5) {5};
  \node at (9, 1.5) {1};
  
  \node at (2, 0.7) {45};
  \node at (4, 0.7) {F};
  \node at (6, 0.7) {9};
  \node at (9, 0.7) {0};
  
  % Add header labels
  \node at (4, 4.2) {\textcolor{datagreen}{\textbf{Candidate predictors}}};
  \node at (9, 4.2) {\textcolor{datacyan}{\textbf{Target}}};
\end{tikzpicture}
\end{center}

\begin{block}{Target Variable}
The outcome variable measured \textcolor{datacyan}{\textbf{after}} the observation point that we aim to predict.
\end{block}

# The Timeline

## The Timeline Concept (1/4)

\begin{center}
\begin{tikzpicture}[scale=1.2]
  % Draw arrow
  \draw[->, line width=3pt, black] (0,0) -- (10,0);
\end{tikzpicture}
\end{center}

\vspace{2em}

\begin{block}{Temporal Structure}
Predictive modeling requires a clear \textbf{temporal separation} between:
\begin{itemize}
  \item \textcolor{datagreen}{\textbf{Past}}: Data used to calculate predictors
  \item \textcolor{datacyan}{\textbf{Future}}: Outcomes to be predicted
\end{itemize}
\end{block}

## The Timeline Concept (2/4)

\begin{center}
\begin{tikzpicture}[scale=1.2]
  % Draw arrow
  \draw[->, line width=3pt, black] (0,0) -- (10,0);
  
  % Add observation date marker
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\small \textbf{May 1st 2018}};
  
  % Add target period
  \fill[datacyan, opacity=0.3] (5,0.3) rectangle (7.5,-0.3);
  \node[below] at (6.25, -0.6) {\small \textcolor{datacyan}{\textbf{Target period (3 months)}}};
  
  % Add end date
  \node[below] at (7.5, 0.5) {\small \textbf{August 1st 2018}};
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{block}{Key Dates}
\begin{itemize}
  \item \textcolor{datared}{\textbf{Observation date}}: Reference point (e.g., mailing date)
  \item \textcolor{datacyan}{\textbf{Target period}}: Window for measuring outcomes
\end{itemize}
\end{block}

## The Timeline Concept (3/4)

\begin{center}
\begin{tikzpicture}[scale=1.2]
  % Draw arrow
  \draw[->, line width=3pt, black] (0,0) -- (10,0);
  
  % Add observation date marker
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\small \textbf{May 1st 2018}};
  
  % Add predictor period
  \fill[datalime, opacity=0.3] (0,0.3) rectangle (5,-0.3);
  \node[below] at (2.5, -0.6) {\small \textcolor{datagreen}{\textbf{Predictive variables}}};
  
  % Add target period
  \fill[datacyan, opacity=0.3] (5,0.3) rectangle (7.5,-0.3);
  \node[below] at (6.25, -0.9) {\small \textcolor{datacyan}{\textbf{Target period}}};
  
  % Add end date
  \node[below] at (7.5, 0.5) {\small \textbf{August 1st 2018}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{alertblock}{Critical Principle}
\textbf{No data leakage}: Predictors must be calculated using \textcolor{datagreen}{\textbf{only}} information available before the observation date.
\end{alertblock}

## The Timeline Concept (4/4)

\begin{block}{Why is Timeline Important?}
\begin{enumerate}
  \item \textbf{Prevents data leakage}: Ensures predictors don't contain future information
  \item \textbf{Mimics deployment}: Replicates real-world prediction scenarios
  \item \textbf{Valid evaluation}: Enables honest assessment of model performance
  \item \textbf{Temporal validity}: Accounts for time-dependent patterns
\end{enumerate}
\end{block}

\begin{exampleblock}{Real-world Example}
To predict donations in May-July 2018 using a mailing sent May 1st, we can only use donor characteristics and behavior from before May 1st.
\end{exampleblock}

# Reconstructing History

## Historical Reconstruction (1/3)

\begin{center}
\begin{tikzpicture}[scale=0.9]
  % First timeline
  \draw[->, line width=2pt, black] (0,2) -- (10,2);
  \node at (5, 2.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 2.8) {\tiny \textbf{May 1st 2018}};
  \fill[datalime, opacity=0.3] (0,2.2) rectangle (5,1.8);
  \node[below] at (2.5, 1.6) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \fill[datacyan, opacity=0.3] (5,2.2) rectangle (7.5,1.8);
  \node[below] at (6.25, 1.6) {\tiny \textcolor{datacyan}{\textbf{Target (3 months)}}};
  \node[below] at (7.5, 2.3) {\tiny \textbf{Aug 1st 2018}};
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{block}{Training Data Construction}
To build robust models, we need \textbf{multiple observation points} from historical data.
\end{block}

## Historical Reconstruction (2/3)

\begin{center}
\begin{tikzpicture}[scale=0.85]
  % First timeline (2018)
  \draw[->, line width=1.5pt, black] (0,3) -- (10,3);
  \node at (5, 3.4) {\small \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 3.6) {\tiny \textbf{May 1st 2018}};
  \fill[datalime, opacity=0.3] (0,3.15) rectangle (5,2.85);
  \node[below] at (2.5, 2.7) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \fill[datacyan, opacity=0.3] (5,3.15) rectangle (7.5,2.85);
  \node[below] at (6.25, 2.7) {\tiny \textcolor{datacyan}{\textbf{Target (3 mo)}}};
  \node[below] at (7.5, 3.2) {\tiny \textbf{Aug 2018}};
  
  % Second timeline (2017)
  \draw[->, line width=1.5pt, black] (0,1.5) -- (9,1.5);
  \node at (5, 1.9) {\small \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 2.1) {\tiny \textbf{May 1st 2017}};
  \fill[datalime, opacity=0.3] (0,1.65) rectangle (5,1.35);
  \node[below] at (2.5, 1.2) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \fill[datacyan, opacity=0.3] (5,1.65) rectangle (7.5,1.35);
  \node[below] at (6.25, 1.2) {\tiny \textcolor{datacyan}{\textbf{Target (3 mo)}}};
  \node[below] at (7.5, 1.7) {\tiny \textbf{Aug 2017}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{block}{Multiple Snapshots}
By shifting the observation date backward, we create additional training samples while maintaining timeline integrity.
\end{block}

## Historical Reconstruction (3/3)

\begin{center}
\begin{tikzpicture}[scale=0.7]
  % Three timelines
  \draw[->, line width=1.2pt, black] (0,4) -- (10,4);
  \node at (5, 4.3) {\footnotesize \textcolor{datared}{$\bigotimes$}};
  \fill[datalime, opacity=0.3] (0,4.1) rectangle (5,3.9);
  \fill[datacyan, opacity=0.3] (5,4.1) rectangle (7.5,3.9);
  \node[above] at (5, 4.5) {\tiny \textbf{May 2018}};
  
  \draw[->, line width=1.2pt, black] (0,2.5) -- (9,2.5);
  \node at (5, 2.8) {\footnotesize \textcolor{datared}{$\bigotimes$}};
  \fill[datalime, opacity=0.3] (0,2.6) rectangle (5,2.4);
  \fill[datacyan, opacity=0.3] (5,2.6) rectangle (7,2.4);
  \node[above] at (5, 3.0) {\tiny \textbf{May 2017}};
  
  % Basetable representation
  \draw[line width=2pt, black] (0,0) rectangle (7,1.5);
  \draw[line width=2pt, black] (5.5,0) -- (5.5,1.5);
  \node at (2.75, 1.8) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \node at (6.25, 1.8) {\tiny \textcolor{datacyan}{\textbf{Target}}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{exampleblock}{Result}
Each historical observation point creates a \textbf{row} in the basetable, increasing sample size for model training.
\end{exampleblock}

## Selecting Relevant Data in R
```{r eval=FALSE, size="tiny"}
# Load and prepare donation data
library(tidyverse)
library(lubridate)

gifts <- read_csv("gifts.csv") %>%
  mutate(date = as.Date(date))

# Define timeline boundaries
start_target <- as.Date("2018-05-01")  # Observation date
end_target <- as.Date("2018-08-01")    # End of target period

# Partition data by timeline
gifts_target <- gifts %>%
  filter(date >= start_target & date < end_target)

gifts_pred_variables <- gifts %>%
  filter(date < start_target)  # Only historical data
```

## R Code: Example Output
```{r echo=FALSE}
# Create sample data
set.seed(123)
gifts <- tibble(
  id = rep(1:5, each = 3),
  date = as.Date(c("2015-10-16", "2014-02-11", "2012-03-28", "2013-12-13", "2012-01-10",
                   "2016-05-20", "2017-03-15", "2018-01-10", "2015-08-22", "2016-11-30",
                   "2017-06-15", "2018-03-20", "2014-09-10", "2016-02-28", "2017-12-05")),
  amount = c(75.0, 111.0, 93.0, 113.0, 93.0, 82.5, 95.0, 105.0, 88.0, 92.5,
             78.0, 110.0, 87.0, 99.0, 103.0)
)
```
```{r size="tiny"}
head(gifts, 5)
```

\vspace{0.5em}

\begin{block}{Data Structure}
Each row represents a \textbf{donation transaction} with donor ID, date, and amount.
\end{block}

# The Population

## Population Definition

\begin{block}{What is the Population?}
The population is the set of \textcolor{datablue}{\textbf{observation units}} (individuals, customers, entities) who are:
\begin{enumerate}
  \item \textbf{Eligible} for the intervention or prediction
  \item \textbf{Available} in the data at the observation date
  \item \textbf{Relevant} to the business problem
\end{enumerate}
\end{block}

\vspace{1em}

\begin{exampleblock}{Example: Donor Prediction}
Population = donors who:
\begin{itemize}
  \item Have a valid mailing address
  \item Have not opted out of communications
  \item Have donated at least once before the observation date
\end{itemize}
\end{exampleblock}

## Population Requirements

\begin{center}
\begin{tikzpicture}[scale=0.65]
  \draw[line width=2pt, black] (0,0) rectangle (10,3);
  \draw[line width=2pt, black] (8,0) -- (8,3);
  
  \node at (-1, 2.3) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 2.3) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  \node at (-1, 1.5) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 1.5) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  \node at (-1, 0.7) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (-0.5, 0.7) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  \node[left] at (-1.5, 1.5) {\textbf{Population}};
  
  \node at (2, 3.5) {\small \textbf{Age}};
  \node at (4, 3.5) {\small \textbf{Gender}};
  \node at (6, 3.5) {\small \textbf{Prev. gifts}};
  \node at (9, 3.5) {\small \textbf{Donate}};
  
  \node at (2, 2.3) {25};
  \node at (4, 2.3) {F};
  \node at (6, 2.3) {12};
  \node at (9, 2.3) {0};
  \node at (2, 1.5) {60};
  \node at (4, 1.5) {M};
  \node at (6, 1.5) {5};
  \node at (9, 1.5) {1};
  \node at (2, 0.7) {45};
  \node at (4, 0.7) {F};
  \node at (6, 0.7) {9};
  \node at (9, 0.7) {0};
  
  \node at (4, 4.2) {\textcolor{datagreen}{\textbf{Candidate predictors}}};
  \node at (9, 4.2) {\textcolor{datacyan}{\textbf{Target}}};
\end{tikzpicture}
\end{center}

\begin{block}{Eligibility Criteria}
\begin{itemize}
  \item Address available \textcolor{datagreen}{}
  \item Privacy settings allow contact \textcolor{datagreen}{}
  \item Active in the system \textcolor{datagreen}{}
\end{itemize}
\end{block}

## Timeline Compliant Population: Age (1/2)

\begin{center}
\begin{tikzpicture}[scale=1.1]
  % Timeline
  \draw[->, line width=2.5pt, black] (0,0) -- (10,0);
  
  % Observation marker
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\small \textbf{May 1st 2018}};
  
  % Periods
  \fill[datalime, opacity=0.3] (0,0.3) rectangle (5,-0.3);
  \node[below] at (2.5, -0.6) {\small \textcolor{datagreen}{\textbf{Predictive variables}}};
  
  \fill[datacyan, opacity=0.3] (5,0.3) rectangle (8,-0.3);
  \node[below] at (6.5, -0.6) {\small \textcolor{datacyan}{\textbf{Target period}}};
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{alertblock}{Temporal Consistency}
Age must be calculated as of the \textcolor{datared}{\textbf{observation date}} (May 1st, 2018), not current age.
\end{alertblock}

## Timeline Compliant Population: Age (2/2)

\begin{center}
\begin{tikzpicture}[scale=1.1]
  % Timeline
  \draw[->, line width=2.5pt, black] (0,0) -- (10,0);
  
  % Observation marker
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\small \textbf{May 1st 2018}};
  
  % Periods
  \fill[datalime, opacity=0.3] (0,0.3) rectangle (5,-0.3);
  \fill[datacyan, opacity=0.3] (5,0.3) rectangle (8,-0.3);
  
  % Person example
  \node at (5, -1.5) {\Large \textcolor{datablue}{$\bigcirc$}};
  \node at (5.5, -1.5) {\Large \textcolor{datablue}{$\widehat{\phantom{X}}$}};
  \node[right] at (6, -1.5) {\textbf{Age 25}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{exampleblock}{Implementation}
\texttt{age\_at\_observation = year(observation\_date) - year(birth\_date)}
\end{exampleblock}

## Timeline Compliant Population: Donations (1/3)

\begin{center}
\begin{tikzpicture}[scale=1.0]
  % Timeline
  \draw[->, line width=2.5pt, black] (0,0) -- (10,0);
  
  % Observation marker
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\small \textbf{May 1st 2018}};
  \node[below] at (7.5, 0.4) {\small \textbf{June 1st 2018}};
  
  % Target period only
  \fill[datacyan, opacity=0.3] (5,0.25) rectangle (7.5,-0.25);
  \node[below] at (6.25, -0.6) {\small \textcolor{datacyan}{\textbf{Target period}}};
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{block}{Population Filtering by Donation History}
To ensure population has potential to donate, we often require \textcolor{datagreen}{\textbf{at least one prior donation}}.
\end{block}

## Timeline Compliant Population: Donations (2/3)

\begin{center}
\begin{tikzpicture}[scale=0.95]
  % Timeline
  \draw[->, line width=2.5pt, black] (0,0) -- (10,0);
  
  % Dates
  \node[below] at (1, 0.4) {\tiny \textbf{Jan 1 2018}};
  \node at (5, 0.5) {\Large \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 0.8) {\tiny \textbf{May 1 2018}};
  \node[below] at (7.5, 0.4) {\tiny \textbf{June 1 2018}};
  
  % Periods
  \fill[datalime, opacity=0.3] (1,0.25) rectangle (5,-0.25);
  \node[below] at (3, -0.55) {\tiny \textcolor{datagreen}{\textbf{At least 1 donation}}};
  
  \fill[gray, opacity=0.2] (5,0.25) rectangle (7.5,-0.25);
  \node[below] at (6.25, -0.55) {\tiny \textbf{No donations}};
  
  \fill[datacyan, opacity=0.3] (7.5,0.25) rectangle (9.5,-0.25);
  \node[below] at (8.5, -0.55) {\tiny \textcolor{datacyan}{\textbf{Target period}}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{block}{Inclusion Criterion}
Include donors with $\geq 1$ donation between Jan 1st and May 1st, but \textbf{exclude} those who donated between May 1st and June 1st.
\end{block}

## Timeline Compliant Population: Donations (3/3)

\begin{center}
\begin{tikzpicture}[scale=0.8]
  % Timeline 2018
  \draw[->, line width=2pt, black] (0,2.5) -- (10,2.5);
  \node[below] at (1, 2.9) {\tiny \textbf{Jan 1 2018}};
  \node at (5, 3.0) {\small \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 3.3) {\tiny \textbf{May 1 2018}};
  \node[below] at (7.5, 2.9) {\tiny \textbf{June 1 2018}};
  \fill[datalime, opacity=0.3] (1,2.65) rectangle (5,2.35);
  \node[below] at (3, 2.2) {\tiny \textcolor{datagreen}{\textbf{At least 1 donation}}};
  \fill[gray, opacity=0.2] (5,2.65) rectangle (7.5,2.35);
  \node[below] at (6.25, 2.2) {\tiny \textbf{No donations}};
  \fill[datacyan, opacity=0.3] (7.5,2.65) rectangle (9.5,2.35);
  \node[below] at (8.5, 2.2) {\tiny \textcolor{datacyan}{\textbf{Target}}};
  
  % Timeline 2017
  \draw[->, line width=2pt, black] (0,1) -- (9,1);
  \node[below] at (1, 1.4) {\tiny \textbf{Jan 1 2017}};
  \node at (5, 1.5) {\small \textcolor{datared}{$\bigotimes$}};
  \node[above] at (5, 1.8) {\tiny \textbf{May 1 2017}};
  \node[below] at (7.5, 1.4) {\tiny \textbf{June 1 2017}};
  \fill[datalime, opacity=0.3] (1,1.15) rectangle (5,0.85);
  \fill[gray, opacity=0.2] (5,1.15) rectangle (7.5,0.85);
  \fill[datacyan, opacity=0.3] (7.5,1.15) rectangle (9.5,0.85);
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{exampleblock}{Multiple Time Points}
Apply the same logic to create populations for 2017, 2016, etc., building a larger training dataset.
\end{exampleblock}

## Population Selection in R: Set Operations
```{r eval=FALSE, size="tiny"}
# Identify donors to INCLUDE (donated in 2016)
donations_2016 <- gifts %>%
  filter(year(date) == 2016)

donors_include <- unique(donations_2016$id)

# Identify donors to EXCLUDE (donated Jan-Apr 2017)
donations_2017_early <- gifts %>%
  filter(year(date) == 2017, month(date) < 5)

donors_exclude <- unique(donations_2017_early$id)

# Population = Include \ Exclude
population <- setdiff(donors_include, donors_exclude)
```

## R Code: Set Operations Example
```{r size="tiny"}
# Create example donor sets
set.seed(456)
donors_include <- c(1002, 3043, 4934, 5012, 7834, 2451, 3047)
donors_exclude <- c(2451, 3047, 4474)

# Apply set difference
population <- setdiff(donors_include, donors_exclude)
population
```

\vspace{0.5em}

\begin{block}{Set Difference Operation}
\texttt{setdiff(A, B)} returns elements in set A that are \textbf{not} in set B.
\end{block}

# The Target Variable

## Target Variable Definition

\begin{block}{What is a Target Variable?}
The target (dependent variable, outcome, label) is the \textcolor{datacyan}{\textbf{quantity we aim to predict}}, measured during the target period.
\end{block}

\vspace{0.5em}

\begin{block}{Types of Targets}
\begin{itemize}
  \item \textbf{Binary}: Did event occur? (Yes/No, 1/0)
  \begin{itemize}
    \item Example: Donated (1) or not (0)
  \end{itemize}
  \item \textbf{Continuous}: What magnitude? (Real number)
  \begin{itemize}
    \item Example: Total donation amount (\$)
  \end{itemize}
  \item \textbf{Categorical}: Which category?
  \begin{itemize}
    \item Example: Customer segment (A, B, C)
  \end{itemize}
\end{itemize}
\end{block}

## Target Timeline (1/3)

\begin{center}
\begin{tikzpicture}[scale=1.1]
  % Timeline
  \draw[->, line width=2.5pt, black] (0,0) -- (10,0);
  
  % Dates
  \node[below] at (5, 0.4) {\small \textbf{August 1 2018}};
  \node[below] at (7.5, 0.4) {\small \textbf{September 1 2018}};
  
  % Periods
  \fill[datalime, opacity=0.3] (0,0.25) rectangle (5,-0.25);
  \node[below] at (2.5, -0.6) {\small \textcolor{datagreen}{\textbf{Predictive variables}}};
  
  \fill[datacyan, opacity=0.3] (5,0.25) rectangle (7.5,-0.25);
  \node[below] at (6.25, -0.6) {\small \textcolor{datacyan}{\textbf{Target period}}};
\end{tikzpicture}
\end{center}

\vspace{1em}

\begin{block}{Target Period Selection}
The target period should:
\begin{itemize}
  \item Be \textcolor{datacyan}{\textbf{actionable}} (e.g., campaign duration)
  \item Match \textcolor{datacyan}{\textbf{business cycle}} (quarterly, monthly)
  \item Provide sufficient \textcolor{datacyan}{\textbf{signal}} (not too short/long)
\end{itemize}
\end{block}

## Target Timeline (2/3)

\begin{center}
\begin{tikzpicture}[scale=0.9]
  % Timeline 2018
  \draw[->, line width=2pt, black] (0,2.5) -- (10,2.5);
  \node[below] at (5, 2.9) {\tiny \textbf{Aug 1 2018}};
  \node[below] at (7.5, 2.9) {\tiny \textbf{Sep 1 2018}};
  \fill[datalime, opacity=0.3] (0,2.65) rectangle (5,2.35);
  \node[below] at (2.5, 2.2) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \fill[datacyan, opacity=0.3] (5,2.65) rectangle (7.5,2.35);
  \node[below] at (6.25, 2.2) {\tiny \textcolor{datacyan}{\textbf{Target period}}};
  
  % Timeline 2017
  \draw[->, line width=2pt, black] (0,1) -- (9,1);
  \node[below] at (5, 1.4) {\tiny \textbf{Aug 1 2017}};
  \node[below] at (7, 1.4) {\tiny \textbf{Sep 1 2017}};
  \fill[datalime, opacity=0.3] (0,1.15) rectangle (5,0.85);
  \node[below] at (2.5, 0.7) {\tiny \textcolor{datagreen}{\textbf{Predictive variables}}};
  \fill[datacyan, opacity=0.3] (5,1.15) rectangle (7,0.85);
  \node[below] at (6, 0.7) {\tiny \textcolor{datacyan}{\textbf{Target period}}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{block}{Consistent Target Definition}
The \textcolor{datacyan}{\textbf{same target definition}} must be applied across all historical observation points for valid model training.
\end{block}

## Target Timeline (3/3)

\begin{center}
\begin{tikzpicture}[scale=0.7]
  % Timeline 2018
  \draw[->, line width=1.5pt, black] (0,3.5) -- (10,3.5);
  \node[below] at (5, 3.9) {\tiny \textbf{Aug 1 2018}};
  \node[below] at (7, 3.9) {\tiny \textbf{Sep 1 2018}};
  \fill[datalime, opacity=0.3] (0,3.65) rectangle (5,3.35);
  \fill[datacyan, opacity=0.3] (5,3.65) rectangle (7,3.35);
  
  % Timeline 2017
  \draw[->, line width=1.5pt, black] (0,2) -- (9,2);
  \node[below] at (5, 2.4) {\tiny \textbf{Aug 1 2017}};
  \node[below] at (6.75, 2.4) {\tiny \textbf{Sep 1 2017}};
  \fill[datalime, opacity=0.3] (0,2.15) rectangle (5,1.85);
  \fill[datacyan, opacity=0.3] (5,2.15) rectangle (6.75,1.85);
  
  % Arrow pointing to basetable
  \draw[->, line width=1.5pt, datared] (5, 1.5) -- (5, 0.8);
  
  % Basetable
  \draw[line width=2pt, black] (0,0) rectangle (9,0.7);
  \draw[line width=2pt, black] (7,0) -- (7,0.7);
  \node at (3.5, 1.0) {\tiny \textcolor{datagreen}{\textbf{Predictors}}};
  \node at (8, 1.0) {\tiny \textcolor{datacyan}{\textbf{Target}}};
\end{tikzpicture}
\end{center}

\vspace{0.5em}

\begin{exampleblock}{Basetable Construction}
Each timeline generates one row in the basetable with historical predictors and future target.
\end{exampleblock}

## Defining Binary Target in R
```{r eval=FALSE, size="tiny"}
# Load target period outcomes (e.g., list of unsubscribers)
unsubscribe_2017 <- c(90112, 65537, 24577, 8196, 73737)

# Create basetable with donor IDs from population
basetable <- tibble(donor_id = population)

# Define binary target: 1 if unsubscribed, 0 otherwise
basetable <- basetable %>%
  mutate(target = if_else(donor_id %in% unsubscribe_2017, 
                          1, 0))
```

\vspace{0.5em}

\begin{alertblock}{Binary Encoding}
\texttt{1} = event occurred (positive class), \texttt{0} = event did not occur (negative class)
\end{alertblock}

## R Code: Binary Target Example
```{r size="tiny"}
# Example binary target creation
unsubscribe_2017 <- c(65537, 65540)
basetable <- tibble(
  donor_id = c(65537, 65538, 65539, 65540, 65541)
)

basetable <- basetable %>%
  mutate(target = if_else(donor_id %in% unsubscribe_2017, 1, 0))

basetable
```

## Defining Aggregated (Continuous) Target in R (1/2)
```{r eval=FALSE, size="tiny"}
# Define target period
start_target <- as.Date("2017-01-01")
end_target <- as.Date("2018-01-01")

# Select donations in target period
gifts_target <- gifts %>%
  filter(date >= start_target & date < end_target)

# Aggregate: sum donations by donor
gifts_target_byid <- gifts_target %>%
  group_by(id) %>%
  summarize(total_amount = sum(amount), .groups = "drop")
```

## Defining Aggregated (Continuous) Target in R (2/2)
```{r eval=FALSE, size="tiny"}
# Define target based on threshold (e.g., donated >$500)
high_value_donors <- gifts_target_byid %>%
  filter(total_amount > 500) %>%
  pull(id)

# Add binary target to basetable
basetable <- basetable %>%
  mutate(target = if_else(donor_id %in% high_value_donors, 
                          1, 0))
```

\vspace{0.5em}

\begin{block}{Aggregation Strategy}
For continuous outcomes, we often \textcolor{dataorange}{\textbf{aggregate}} transactions (sum, mean, count) within the target period, then potentially \textcolor{dataorange}{\textbf{threshold}} to create binary targets.
\end{block}

# Summary

## Key Takeaways

\begin{block}{Core Concepts}
\begin{enumerate}
  \item \textbf{Basetable}: Structured matrix with observations (rows) and variables (columns)
  \item \textbf{Timeline}: Temporal separation between predictor calculation and target measurement
  \item \textbf{Population}: Eligible observation units defined by business rules
  \item \textbf{Target}: Outcome variable measured in the target period
  \item \textbf{Historical reconstruction}: Multiple observation points create training samples
\end{enumerate}
\end{block}

## Critical Principles

\begin{alertblock}{Golden Rules}
\begin{itemize}
  \item \textcolor{datared}{\textbf{No data leakage}}: Predictors use only pre-observation data
  \item \textcolor{datared}{\textbf{Consistent definitions}}: Same target/population logic across time
  \item \textcolor{datared}{\textbf{Timeline integrity}}: Maintain temporal ordering in all operations
  \item \textcolor{datared}{\textbf{Eligibility criteria}}: Population must be actionable
\end{itemize}
\end{alertblock}

## Practical Implementation Steps

\begin{enumerate}
  \item Define \textcolor{dataorange}{\textbf{business problem}} and target outcome
  \item Establish \textcolor{dataorange}{\textbf{observation dates}} and target periods
  \item Specify \textcolor{dataorange}{\textbf{population eligibility}} criteria
  \item Partition \textcolor{dataorange}{\textbf{data by timeline}} (predictors vs. target)
  \item Calculate \textcolor{dataorange}{\textbf{features}} from historical data
  \item Define and measure \textcolor{dataorange}{\textbf{target variable}}
  \item Construct \textcolor{dataorange}{\textbf{final basetable}}
  \item Validate \textcolor{dataorange}{\textbf{temporal integrity}}
\end{enumerate}

## Next Steps

\begin{block}{Coming Up}
In the next lecture, we will cover:
\begin{itemize}
  \item \textbf{Feature engineering}: Creating predictive variables from raw data
  \item \textbf{Aggregation techniques}: RFM (Recency, Frequency, Monetary) features
  \item \textbf{Handling missing data}: Imputation strategies
  \item \textbf{Feature selection}: Identifying the most predictive variables
\end{itemize}
\end{block}

\vspace{1em}

\begin{center}
\textcolor{datablue}{\Large \textbf{Questions?}}
\end{center}

# Appendix

## Additional Resources

\begin{block}{Recommended Reading}
\begin{itemize}
  \item Verbiest, N. et al. (2018). "Building Maintainable Credit Scoring Models Using Time-Consistent Strategies"
  \item Provost, F., \& Fawcett, T. (2013). \textit{Data Science for Business}. O'Reilly Media.
  \item Kuhn, M., \& Johnson, K. (2019). \textit{Feature Engineering and Selection: A Practical Approach for Predictive Models}. CRC Press.
\end{itemize}
\end{block}

\begin{block}{R Packages}
\begin{itemize}
  \item \texttt{tidyverse}: Data manipulation and visualization
  \item \texttt{lubridate}: Date-time handling
  \item \texttt{recipes}: Feature engineering framework
\end{itemize}
\end{block}

## Practice Exercise

\begin{exampleblock}{Exercise: Construct a Basetable}
Given a dataset of customer transactions:
\begin{enumerate}
  \item Define an observation date (e.g., 2019-06-01)
  \item Create a 3-month target period
  \item Filter population: customers with $\geq$ 2 purchases before observation
  \item Calculate predictor: total spending before observation date
  \item Define binary target: purchased during target period (1/0)
  \item Construct final basetable
\end{enumerate}
\end{exampleblock}

\vspace{0.5em}

\begin{block}{Deliverable}
A basetable with columns: \texttt{customer\_id}, \texttt{total\_spending}, \texttt{target}
\end{block}


# The Feature Engineering Mindset

## Think Like a Detective

**Raw Data Says:** "Donor 123 gave €50 last month"

**Good Features Ask:**

1. **Recency:** How long ago? (Yesterday? Last year?)
2. **Frequency:** Is this typical? (First time? Monthly ritual?)
3. **Monetary:** Generous or modest? (More than usual? Less?)
4. **Trend:** What's the direction? (Increasing? Decreasing?)
5. **Context:** What else matters? (Season? Life event?)

\vspace{1em}

**Real Example:** Two donors, both gave €100 this year

- **Donor A:** €10 × 10 times → Consistent supporter 
- **Donor B:** €100 × 1 time → One-time gift ?

**Same total, different stories!** Features capture these nuances.

---

# Part 1: Multi-Window Aggregation

## Why Multiple Time Windows?

**The Problem:** Behavior changes at different speeds
```{r, eval=FALSE}
# Three windows, three perspectives
library(lubridate)
reference_date <- as.Date("2024-01-01")

# Recent behavior (3 months)
window_3m <- gifts %>% 
  filter(date >= reference_date - months(3),
         date < reference_date)

# Medium-term pattern (12 months)  
window_12m <- gifts %>% 
  filter(date >= reference_date - months(12),
         date < reference_date)

# Long-term history (24 months)
window_24m <- gifts %>% 
  filter(date >= reference_date - months(24),
         date < reference_date)
```

**Why this matters:** Recent spike? Or sustained increase? Windows reveal the truth.

---

## Window Selection: The Goldilocks Problem

**Too Short (1 month):**

- Captures noise, not signal
- Sensitive to one-off events
- Example: "Donor gave last week because of emergency appeal"

**Too Long (5 years):**

- Ancient history dominates
- Misses recent changes  
- Example: "Used to give a lot, but stopped 2 years ago"

**Just Right (3-12 months):**

- Balances recency and stability
- Captures true behavior patterns
- Aligns with business planning cycles

\vspace{1em}

\begin{center}
\textcolor{datablue}{\textbf{Rule of Thumb:}} Match your window to your prediction horizon\\
Predicting next month? Use 3-6 month features\\
Predicting next year? Use 12-24 month features
\end{center}

---

## Building the Multi-Window Dataset
```{r, eval=FALSE}
# Aggregate each window separately
agg_3m <- window_3m %>% 
  group_by(donor_id) %>% 
  summarise(
    donations_3m = sum(amount),    # Total given
    count_3m = n(),                # How many times
    avg_3m = mean(amount)          # Average gift
  )

agg_12m <- window_12m %>% 
  group_by(donor_id) %>% 
  summarise(
    donations_12m = sum(amount),
    count_12m = n(),
    avg_12m = mean(amount)
  )

# Combine into basetable
basetable <- basetable %>% 
  left_join(agg_3m, by = "donor_id") %>% 
  left_join(agg_12m, by = "donor_id")
```

**Note:** `left_join` keeps all donors, even those with zero activity!

---

## Quick Example: Window Comparison
```{r, echo=FALSE, eval=FALSE}
# Example output (for slide visualization)
example_output <- tibble(
  donor_id = c(101, 102, 103),
  donations_3m = c(150, 0, 300),
  donations_12m = c(500, 400, 350),
  count_3m = c(3, 0, 5),
  count_12m = c(10, 8, 6)
)
```

| Donor | 3m Total | 12m Total | Interpretation |
|-------|----------|-----------|----------------|
| 101   | €150     | €500      | Slowing down (30% of annual in recent quarter) |
| 102   | €0       | €400      | Went quiet recently! \textbf{(!)} |
| 103   | €300     | €350      | Accelerating! (86% of annual in last 3m) |

**The magic:** Comparing windows reveals **momentum**

---

# Part 2: RFM - The Holy Trinity

## Recency, Frequency, Monetary

**RFM:** The three pillars of behavioral prediction

\begin{center}
\begin{tikzpicture}[scale=0.8]
% Three pillars
\fill[datablue!30] (0,0) rectangle (2,4);
\fill[datagreen!30] (3,0) rectangle (5,4);
\fill[datared!30] (6,0) rectangle (8,4);

\node at (1,2) [align=center] {\textbf{Recency}\\\vspace{0.5em}How\\recently?};
\node at (4,2) [align=center] {\textbf{Frequency}\\\vspace{0.5em}How\\often?};
\node at (7,2) [align=center] {\textbf{Monetary}\\\vspace{0.5em}How\\much?};

% Base
\fill[black!20] (-0.5,-0.3) rectangle (8.5,0);
\node at (4,-0.6) {\textbf{Behavioral Foundation}};
\end{tikzpicture}
\end{center}

**Why RFM works:** These three capture fundamentally different aspects of engagement

---

## Recency: The Clock is Ticking

**Concept:** Time since last donation predicts next donation
```{r, eval=FALSE}
# Calculate days since last gift
recency <- gifts %>% 
  filter(date < reference_date) %>% 
  group_by(donor_id) %>% 
  summarise(last_gift_date = max(date)) %>% 
  mutate(
    days_since = as.numeric(reference_date - last_gift_date),
    
    # Create meaningful categories
    recency_segment = case_when(
      days_since <= 30  ~ "Active",     # Gave last month
      days_since <= 90  ~ "Warm",       # Gave this quarter
      days_since <= 365 ~ "Cooling",    # Gave this year
      TRUE ~ "Cold"                     # Over a year ago
    )
  )
```

**Business insight:** "Active" donors are 5× more likely to give again than "Cold" donors

---

## Frequency: Habits Matter

**Concept:** Past frequency predicts future frequency
```{r, eval=FALSE}
# How often do they give?
frequency <- gifts %>% 
  filter(date >= reference_date - months(12),
         date < reference_date) %>% 
  group_by(donor_id) %>% 
  summarise(
    gift_count = n(),
    unique_months = n_distinct(floor_date(date, "month")),
    
    # Calculate regularity
    regularity = unique_months / 12  # Score 0-1
  ) %>% 
  mutate(
    frequency_segment = case_when(
      gift_count >= 12 ~ "Monthly",      # Every month
      gift_count >= 4  ~ "Regular",      # Quarterly
      gift_count >= 2  ~ "Occasional",   # Semi-annual
      TRUE ~ "Rare"                      # Once a year
    )
  )
```

**Surprise finding:** "Regular" donors (4-11 gifts/year) have highest retention!

---

## Monetary: Show Me the Money
```{r, eval=FALSE}
# How much do they give?
monetary <- gifts %>% 
  filter(date >= reference_date - months(12),
         date < reference_date) %>% 
  group_by(donor_id) %>% 
  summarise(
    total_value = sum(amount),
    avg_gift = mean(amount),
    max_gift = max(amount),
    
    # Variability matters too!
    cv = sd(amount) / mean(amount)  # Coefficient of variation
  ) %>% 
  mutate(
    value_tier = case_when(
      total_value >= 1000 ~ "Major",      # €1000+
      total_value >= 500  ~ "Premium",    # €500-1000
      total_value >= 100  ~ "Standard",   # €100-500
      TRUE ~ "Entry"                      # <€100
    )
  )
```

**Key metric:** `cv` tells us consistency → low CV = predictable giving

---

## Combining RFM: Segmentation
```{r, eval=FALSE}
# Join all three components
rfm <- basetable %>% 
  left_join(recency %>% select(donor_id, days_since), 
            by = "donor_id") %>% 
  left_join(frequency %>% select(donor_id, gift_count), 
            by = "donor_id") %>% 
  left_join(monetary %>% select(donor_id, total_value), 
            by = "donor_id") %>% 
  mutate(
    # Score each dimension 1-5 (5 = best)
    R_score = ntile(-days_since, 5),   # Negative: recent = high score
    F_score = ntile(gift_count, 5),
    M_score = ntile(total_value, 5),
    
    # Combined RFM code (e.g., "555" = best)
    RFM_segment = paste0(R_score, F_score, M_score)
  )
```

**Champion segment:** RFM = "555" → Recent, Frequent, High-Value donors!

---

## RFM in Action: Real Segments

| Segment | R | F | M | Label | Strategy |
|---------|---|---|---|-------|----------|
| 555 | 5 | 5 | 5 | Champions | Cultivate & thank |
| 511 | 5 | 1 | 1 | New Enthusiasts | Nurture relationship |
| 155 | 1 | 5 | 5 | At Risk | Win-back campaign \textbf{(!)} |
| 111 | 1 | 1 | 1 | Lost | Don't waste resources |

**Marketing insight:** Segment 155 (At Risk) → immediate intervention!

**Cost savings:** Don't mail Segment 111 → save 40% of mailing costs

---

# Part 3: The Power of Trends

## Static vs. Dynamic Features

**Problem with snapshots:** They miss the movie

\begin{center}
\begin{tikzpicture}[scale=0.7]
% Timeline
\draw[->,thick] (0,0) -- (10,0) node[right] {Time};

% Donor A (stable)
\foreach \x in {1,2,3,4,5,6,7,8} {
  \fill[datablue] (\x,2) circle (0.1);
}
\node at (4.5,2.7) {Donor A: €50 each time};

% Donor B (increasing)
\fill[datagreen] (1,0.5) circle (0.1);
\fill[datagreen] (2,0.7) circle (0.1);
\fill[datagreen] (3,0.9) circle (0.1);
\fill[datagreen] (4,1.2) circle (0.1);
\fill[datagreen] (5,1.5) circle (0.1);
\fill[datagreen] (6,1.8) circle (0.1);
\node at (4,0) [below] {Donor B: Growing!};
\end{tikzpicture}
\end{center}

**Both gave €300 total → but very different futures!**

**Solution:** Calculate **change rates** and **trends**

---

## Building Trend Features: The Recipe
```{r, eval=FALSE}
# Step 1: Define comparison periods
recent_3m <- gifts %>% 
  filter(date >= reference_date - months(3),
         date < reference_date)

previous_3m <- gifts %>% 
  filter(date >= reference_date - months(6),
         date < reference_date - months(3))

# Step 2: Aggregate each period
recent_agg <- recent_3m %>% 
  group_by(donor_id) %>% 
  summarise(recent_total = sum(amount))

previous_agg <- previous_3m %>% 
  group_by(donor_id) %>% 
  summarise(previous_total = sum(amount))

# Step 3: Calculate change
trends <- recent_agg %>% 
  full_join(previous_agg, by = "donor_id") %>% 
  mutate(
    recent_total = replace_na(recent_total, 0),
    previous_total = replace_na(previous_total, 0),
    
    absolute_change = recent_total - previous_total,
    percent_change = (recent_total - previous_total) / 
      (previous_total + 1)  # +1 prevents division by zero
  )
```

---

## Interpreting Trend Signals
```{r, echo=FALSE, eval=FALSE}
# Example trends
trend_examples <- tibble(
  donor = c("Alice", "Bob", "Carol"),
  previous = c(100, 200, 0),
  recent = c(200, 150, 50),
  change_pct = c(100, -25, Inf)
)
```

| Donor | Previous | Recent | Change | Signal |
|-------|----------|--------|--------|--------|
| Alice | €100 | €200 | +100% | $\nearrow$ Accelerating |
| Bob   | €200 | €150 | -25%  | $\searrow$ Declining |
| Carol | €0   | €50  | New!  | $\star$ Emerging |

**Actionable insights:**

- **Alice:** Ready for upgrade ask (€300?)
- **Bob:** Investigate decline (contact them!)
- **Carol:** Welcome series (nurture new behavior)

---

## Categorizing Trends
```{r, eval=FALSE}
# Create interpretable trend categories
trends <- trends %>% 
  mutate(
    trend_category = case_when(
      previous_total == 0 & recent_total > 0 ~ "New Active",
      percent_change > 0.25 ~ "Strong Growth",
      percent_change > 0 ~ "Modest Growth",
      percent_change > -0.25 ~ "Slight Decline",
      percent_change > -0.5 ~ "Moderate Decline",
      TRUE ~ "Sharp Decline"
    ),
    
    # Binary flag for action
    needs_attention = percent_change < -0.25
  )
```

**Why categories?** Easier for business users to understand and act on

**Pro tip:** Create `needs_attention` flags → automatic alerts to fundraising team

---

# Part 4: Seasonality Matters

## The Calendar Effect

**Real phenomenon:** Donations spike in December (end-of-year tax planning)

\begin{center}
\begin{tikzpicture}[scale=0.7]
\draw[->] (0,0) -- (13,0) node[right] {Month};
\draw[->] (0,0) -- (0,4) node[above] {Donations};

% Monthly pattern
\draw[thick,datablue] 
  (1,1) -- (2,0.9) -- (3,1.1) -- (4,1) -- (5,0.8) -- (6,0.9) --
  (7,0.85) -- (8,0.8) -- (9,1.2) -- (10,1.4) -- (11,1.8) -- (12,3.5);

% Highlight December
\fill[datared,opacity=0.3] (11.5,0) rectangle (12.5,4);
\node at (12,3.8) {\textbf{Dec}};

\foreach \x in {1,2,...,12} {
  \draw (\x,0.05) -- (\x,-0.05);
}
\end{tikzpicture}
\end{center}

**Problem:** December total isn't comparable to July total!

**Solution:** Calculate **seasonal indices**

---

## Calculating Seasonal Indices
```{r, eval=FALSE}
# Extract historical seasonal patterns (exclude recent year)
seasonal_history <- gifts %>% 
  filter(date < reference_date - years(1),
         date >= reference_date - years(3)) %>% 
  mutate(month = month(date)) %>% 
  group_by(donor_id, month) %>% 
  summarise(avg_monthly = mean(amount), .groups = "drop")

# Calculate index: ratio to annual average
seasonal_indices <- seasonal_history %>% 
  group_by(donor_id) %>% 
  mutate(
    annual_avg = mean(avg_monthly),
    seasonal_index = avg_monthly / annual_avg
  )

# Extract current month's index
current_month <- month(reference_date)
donor_seasonality <- seasonal_indices %>% 
  filter(month == current_month) %>% 
  select(donor_id, seasonal_index)
```

**Interpretation:** Index = 1.5 means "this month is typically 50% above average"

---

## Adjusting for Seasonality
```{r, eval=FALSE}
# Add seasonal adjustment to features
basetable <- basetable %>% 
  left_join(donor_seasonality, by = "donor_id") %>% 
  mutate(
    # If missing seasonality data, assume neutral (1.0)
    seasonal_index = replace_na(seasonal_index, 1.0),
    
    # Adjust recent donations for fair comparison
    donations_3m_adjusted = donations_3m / seasonal_index,
    
    # Compare adjusted values to annual average
    performance_vs_seasonal = donations_3m_adjusted / 
      (donations_12m / 4)  # Quarterly average
  )
```

**Business value:** "Bob gave €300 in July (low season, index=0.8) → adjusted = €375 → Actually performing well!"

---

# Part 5: Feature Interactions

## When 1 + 1 = 3

**Concept:** Features are more powerful combined than alone

**Example:** Recency × Frequency interaction
```{r, eval=FALSE}
# Create interaction terms
basetable <- basetable %>% 
  mutate(
    # Recency-Frequency: Recent × Frequent = highly engaged
    RF_interaction = (1 / (days_since + 1)) * gift_count,
    
    # Frequency-Monetary: High frequency + High value = premium
    FM_interaction = gift_count * avg_gift,
    
    # Trend-Level: Growing + Large = invest more attention
    trend_strength = abs(percent_change) * total_value
  )
```

---

## Interaction Example: RF Score

| Donor | Days Since | Count | RF Score | Interpretation |
|-------|------------|-------|----------|----------------|
| A | 10 | 12 | 1.09 | Recent & frequent = **Best!** $\star$ |
| B | 10 | 2  | 0.18 | Recent but infrequent |
| C | 365 | 12 | 0.03 | Frequent but not recent \textbf{(!)} |
| D | 365 | 2  | 0.005 | Neither recent nor frequent |

**Key insight:** Donor C looks good on frequency alone, but RF interaction reveals the problem!

**Model benefit:** Interaction terms help models learn these nuances automatically

---

## Ratio Features: Relative Measures
```{r, eval=FALSE}
# Create ratio-based features
basetable <- basetable %>% 
  mutate(
    # Evolution: is recent behavior above or below average?
    ratio_3m_to_12m = donations_3m / (donations_12m + 0.01),
    
    # Concentration: does one big gift dominate?
    max_to_total_ratio = max_gift / (total_value + 0.01),
    
    # Consistency: how variable are gift sizes?
    consistency_score = 1 - (cv_donation / 2),  # Scaled 0-1
    
    # Lifetime value rate
    lifetime_intensity = total_value / 
      as.numeric(reference_date - member_since) * 365
  )
```

**Why ratios?** They're **scale-invariant** → work for small and large donors

---

# Part 6: Handling Missing Values

## The Two Types of Missing

**Type 1: "No Data"** → Donor joined after the window started

**Type 2: "No Activity"** → Donor didn't give during the window
```{r, eval=FALSE}
# Smart imputation strategy
basetable <- basetable %>% 
  mutate(
    # Flag the reason for missingness
    is_new_donor = as.numeric(reference_date - member_since) < 90,
    
    # Different imputation by reason
    donations_12m = case_when(
      is_new_donor & is.na(donations_12m) ~ NA_real_,  # Keep NA
      is.na(donations_12m) ~ 0,                        # Zero activity
      TRUE ~ donations_12m
    ),
    
    # For ratios, handle zero denominators
    ratio_3m_to_12m = case_when(
      donations_12m == 0 ~ NA_real_,  # Can't calculate
      TRUE ~ donations_3m / donations_12m
    )
  )
```

**Why this matters:** Model interprets NA vs. 0 differently!

---

## Missing Value Indicators
```{r, eval=FALSE}
# Create "missingness flags" as features
basetable <- basetable %>% 
  mutate(
    # Flag no recent activity
    flag_inactive_3m = as.integer(donations_3m == 0),
    flag_inactive_12m = as.integer(donations_12m == 0),
    
    # Flag new donor status
    flag_new_donor = as.integer(is_new_donor),
    
    # Flag data quality issues
    flag_incomplete_history = as.integer(
      as.numeric(reference_date - member_since) < 365 & 
      !is_new_donor
    )
  )
```

**Why flags?** They're features themselves! "No activity" is predictive.

---

# Part 7: Feature Binning

## From Continuous to Categories

**Why bin?** Sometimes categories capture non-linear relationships better
```{r, eval=FALSE}
# Create bins using quantiles (equal population)
basetable <- basetable %>% 
  mutate(
    # Donation frequency bins
    freq_bin = cut(
      gift_count,
      breaks = quantile(gift_count, probs = seq(0, 1, 0.25),
                       na.rm = TRUE),
      labels = c("Q1-Low", "Q2", "Q3", "Q4-High"),
      include.lowest = TRUE
    ),
    
    # Recency bins (business-defined)
    recency_bin = cut(
      days_since,
      breaks = c(0, 30, 90, 180, 365, Inf),
      labels = c("0-30d", "31-90d", "3-6m", "6-12m", "12m+"),
      right = FALSE
    )
  )
```

---

## Binning Strategies Compared

**Quantile bins:** Equal population in each bin

- Pro: Handles outliers well
- Con: Bin boundaries change over time

**Fixed bins:** Domain-knowledge boundaries

- Pro: Stable, interpretable
- Con: May have very unequal populations

**Example decision:** Use fixed bins for **recency** (business naturally thinks in months), quantiles for **monetary value** (wide range)

---

# Part 8: Putting It All Together

## The Complete Feature Set
```{r, eval=FALSE}
# Final feature engineering pipeline
create_features <- function(gifts, basetable, reference_date) {
  
  # 1. RFM features
  rfm_features <- calculate_rfm(gifts, reference_date)
  
  # 2. Trend features  
  trend_features <- calculate_trends(gifts, reference_date)
  
  # 3. Seasonal adjustments
  seasonal_features <- calculate_seasonality(gifts, reference_date)
  
  # 4. Interaction terms
  basetable <- basetable %>% 
    left_join(rfm_features, by = "donor_id") %>% 
    left_join(trend_features, by = "donor_id") %>% 
    left_join(seasonal_features, by = "donor_id") %>% 
    mutate(
      # Interactions
      RF_score = (1 / (days_since + 1)) * gift_count,
      # ... more interactions ...
      
      # Ratios
      ratio_3m_to_12m = donations_3m / (donations_12m + 0.01),
      # ... more ratios ...
    )
  
  return(basetable)
}
```

---

## Feature Checklist

Before moving to modeling, verify:

- [ ] All features use **only past data** (before reference date)
- [ ] Missing values handled **appropriately** (not arbitrarily)
- [ ] Outliers **capped or winsorized** (if needed)
- [ ] Categorical variables **encoded** (if using tree models)
- [ ] Feature names are **clear and documented**
- [ ] Temporal **stability checked** (do features exist at all time points?)

**Red flag:** Feature has 50% missing values → investigate before using!

**Green light:** Feature has clear business meaning and predictive logic

---

# Part 9: Feature Selection

## Not All Features Are Created Equal

**Problem:** 100+ features → some are redundant or noisy
```{r, eval=FALSE}
# Method 1: Correlation filtering
library(caret)

# Remove highly correlated features
feature_matrix <- basetable %>% 
  select(where(is.numeric)) %>% 
  select(-donor_id)

cor_matrix <- cor(feature_matrix, use = "complete.obs")
high_cor <- findCorrelation(cor_matrix, cutoff = 0.90)

features_to_drop <- names(feature_matrix)[high_cor]
```

**Why?** If two features are 95% correlated, we only need one!

**Example:** `donations_12m` and `avg_gift * count_12m` → redundant

---

## Importance-Based Selection
```{r, eval=FALSE}
# Method 2: Random Forest importance
library(randomForest)

# Fit initial model
rf_model <- randomForest(
  target ~ .,
  data = basetable %>% select(-donor_id),
  importance = TRUE,
  ntree = 100
)

# Extract importance scores
importance_df <- importance(rf_model) %>% 
  as.data.frame() %>% 
  rownames_to_column("feature") %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  head(20)  # Keep top 20

print(importance_df)
```

**Result:** Focus on features that actually drive predictions!

---

## Example: Top 10 Features

| Rank | Feature | Importance | Interpretation |
|------|---------|------------|----------------|
| 1 | days_since | 245 | Recency dominates! |
| 2 | donations_12m | 189 | Total value matters |
| 3 | RF_interaction | 156 | Interaction helps |
| 4 | trend_category | 134 | Momentum signal |
| 5 | gift_count | 98 | Frequency counts |
| 6 | ratio_3m_to_12m | 87 | Recent behavior |
| 7 | seasonal_index | 72 | Context matters |
| 8 | max_gift | 65 | Capacity indicator |
| 9 | cv_donation | 54 | Consistency signal |
| 10 | lifetime_days | 48 | Tenure relevant |

**Surprise:** Demographics (age, gender) ranked 25+!

---

# Part 10: Validation Strategy

## Walk-Forward Cross-Validation

**Problem:** Random CV violates timeline!

**Solution:** Walk forward through time

\begin{center}
\begin{tikzpicture}[scale=0.6]
\foreach \i in {1,2,3} {
  \pgfmathsetmacro{\trainstart}{0}
  \pgfmathsetmacro{\trainend}{2*\i}
  \pgfmathsetmacro{\teststart}{2*\i}
  \pgfmathsetmacro{\testend}{2*\i+1}
  
  \fill[datagreen!30] (\trainstart,{-1.5*\i}) rectangle (\trainend,{-1.5*\i+0.8});
  \fill[datacyan!50] (\teststart,{-1.5*\i}) rectangle (\testend,{-1.5*\i+0.8});
  
  \node at ({(\trainstart+\trainend)/2},{-1.5*\i+0.4}) {Train};
  \node at ({(\teststart+\testend)/2},{-1.5*\i+0.4}) {Test};
  \node at (-1,{-1.5*\i+0.4}) {Fold \i};
}

\draw[->] (0,-5) -- (8,-5) node[right] {Time};
\end{tikzpicture}
\end{center}

**Key:** Test set always **after** training set → realistic evaluation

---

## Implementing Walk-Forward CV
```{r, eval=FALSE}
# Create temporal splits
walk_forward_splits <- function(data, n_splits = 3) {
  n_obs <- nrow(data)
  fold_size <- floor(n_obs / (n_splits + 1))
  
  splits <- list()
  
  for(i in 1:n_splits) {
    train_idx <- 1:(fold_size * i)
    test_idx <- (fold_size * i + 1):(fold_size * (i + 1))
    
    splits[[i]] <- list(
      train = data[train_idx, ],
      test = data[test_idx, ]
    )
  }
  
  return(splits)
}

# Usage
splits <- walk_forward_splits(basetable, n_splits = 3)
```

---

## Evaluating Performance
```{r, eval=FALSE}
# Train and evaluate on each fold
library(pROC)

cv_results <- map_df(1:length(splits), function(i) {
  # Train model
  model <- glm(
    target ~ days_since + donations_12m + RF_score,
    data = splits[[i]]$train,
    family = "binomial"
  )
  
  # Predict on test set
  predictions <- predict(model, splits[[i]]$test, type = "response")
  
  # Calculate AUC
  auc_value <- auc(roc(splits[[i]]$test$target, predictions))
  
  tibble(
    fold = i,
    auc = as.numeric(auc_value),
    n_train = nrow(splits[[i]]$train),
    n_test = nrow(splits[[i]]$test)
  )
})

print(cv_results)
# Average AUC across folds = true performance estimate
```

---

# Part 11: Feature Documentation

## Future-You Will Thank You

**Problem:** 6 months later, "What does `var_x47` mean?"

**Solution:** Document everything!
```{r, eval=FALSE}
# Create feature catalog
feature_catalog <- tibble(
  feature_name = c(
    "donations_12m",
    "RF_interaction",
    "ratio_3m_to_12m"
  ),
  description = c(
    "Total donation value in 12 months before reference date",
    "Interaction: recency × frequency for engagement score",
    "Proportion of annual donations made in recent quarter"
  ),
  calculation = c(
    "sum(amount) WHERE date IN [ref-12m, ref)",
    "(1 / (days_since + 1)) * gift_count",
    "donations_3m / donations_12m"
  ),
  missing_handling = c(
    "Replace with 0 if no activity",
    "Requires both components; NA if either missing",
    "Set NA if denominator is 0"
  ),
  added_date = c(
    "2024-01-15",
    "2024-02-01",
    "2024-02-01"
  )
)
```

---

## Feature Catalog Example

| Feature | Type | Window | Business Meaning |
|---------|------|--------|------------------|
| days_since | Numeric | Point-in-time | Days since last donation (recency) |
| donations_12m | Numeric | 12 months | Total annual contribution |
| RF_score | Numeric | Derived | Combined engagement metric |
| trend_category | Categorical | 3m vs 3m | Direction of behavior change |

**Pro tip:** Export as CSV, share with business stakeholders

**Bonus:** Helps detect errors (e.g., "Wait, this calculation doesn't match reality!")

---

# Part 12: Production Considerations

## From Notebook to Pipeline

**Development:** Code runs once on historical data

**Production:** Code runs repeatedly on new data
```{r, eval=FALSE}
# Parameterized feature engineering
engineer_features <- function(reference_date, 
                               gifts_data,
                               basetable_data) {
  
  # Use parameters, not hardcoded dates!
  window_3m_start <- reference_date - months(3)
  window_12m_start <- reference_date - months(12)
  
  # Filter data
  recent_gifts <- gifts_data %>% 
    filter(date >= window_3m_start, date < reference_date)
  
  # Calculate features
  features <- calculate_all_features(
    recent_gifts, 
    window_start = window_12m_start,
    window_end = reference_date
  )
  
  # Join to basetable
  result <- basetable_data %>% 
    left_join(features, by = "donor_id")
  
  return(result)
}
```

**Key:** Everything is a **function** with **parameters**

---

## Testing Your Pipeline
```{r, eval=FALSE}
# Unit tests catch bugs early
library(testthat)

test_that("Features respect timeline", {
  # Create test data
  test_gifts <- tibble(
    donor_id = 1,
    date = as.Date(c("2023-06-01", "2024-01-15")),
    amount = c(100, 50)
  )
  
  ref_date <- as.Date("2024-01-01")
  
  # Run feature engineering
  features <- engineer_features(ref_date, test_gifts, basetable)
  
  # Assert: Only June gift should count
  expect_equal(features$donations_12m[features$donor_id == 1], 100)
  expect_equal(features$gift_count[features$donor_id == 1], 1)
})
```

**Saves hours:** Catches timeline leakage immediately!

---

## Monitoring in Production

**Track feature drift:**
```{r, eval=FALSE}
# Compare distributions over time
monitor_features <- function(new_data, baseline_data) {
  
  features_to_monitor <- c("donations_12m", "days_since", "RF_score")
  
  drift_report <- map_df(features_to_monitor, function(feat) {
    # KS test for distribution change
    ks_result <- ks.test(
      baseline_data[[feat]],
      new_data[[feat]]
    )
    
    tibble(
      feature = feat,
      ks_statistic = ks_result$statistic,
      p_value = ks_result$p.value,
      drift_detected = ks_result$p.value < 0.05,
      baseline_mean = mean(baseline_data[[feat]], na.rm = TRUE),
      current_mean = mean(new_data[[feat]], na.rm = TRUE)
    )
  })
  
  return(drift_report)
}
```

**Action:** Alert if p-value < 0.05 → investigate!

---

# Summary: Feature Engineering Principles

## The Ten Commandments

1. **Timeline Compliance:** Never use future data
2. **Multiple Windows:** Short-term and long-term perspectives
3. **RFM Always:** Recency, Frequency, Monetary are foundational
4. **Capture Trends:** Change matters more than level
5. **Context Matters:** Seasonality and life stage
6. **Interactions:** 1 + 1 can equal 3
7. **Handle Missing:** Distinguish "no data" from "no activity"
8. **Document Everything:** Future-you will thank present-you
9. **Validate Temporally:** Walk forward, don't shuffle
10. **Monitor Production:** Features drift, models decay

---

## From Features to Predictions

**Next Steps:**

1. **Feature Selection:** Keep top 20-30 features
2. **Model Training:** Logistic regression → Random Forest → Gradient Boosting
3. **Hyperparameter Tuning:** Grid search with CV
4. **Model Evaluation:** AUC, calibration, business metrics
5. **Deployment:** API for scoring new donors
6. **Monitoring:** Track performance decay

\vspace{1em}

**Remember:** 

\begin{center}
\textcolor{datablue}{\Large\textbf{Better features > Fancier models}}
\end{center}

Spend 80% of time on feature engineering, 20% on model selection!

---

## Real Impact Story

**Organization:** International humanitarian NGO

**Challenge:** Retain monthly donors (50% churned within 1 year)

**Solution:** Built features tracking:

- RFM scores
- Donation trends (3m vs 12m)
- Seasonal patterns
- Email engagement × donation frequency

**Results:**

- **AUC:** 0.58 → 0.74 (massive improvement!)
- **Business impact:** Identified 8% of donors representing 40% of churn risk
- **Intervention:** Personalized outreach → 15% churn reduction
- **ROI:** €450K saved in first year

**Key insight:** Trend features (growth/decline) were most predictive!

---

## Resources for Deep Dive

**Books:**

- *Feature Engineering for Machine Learning* by Zheng & Casari
- *Feature Engineering and Selection* by Kuhn & Johnson

**Online:**

- Kaggle: "Feature Engineering" courses
- Towards Data Science: Time series feature engineering

**R Packages:**

- `recipes`: Feature engineering pipeline
- `timetk`: Time series features
- `caret`: Feature selection

**Key Principle:** Domain knowledge + creativity + validation = great features

---

## Practical Exercise (For Next Session)

**Dataset:** Provided donor transaction data

**Task:** Create these features:

1. RFM scores (R, F, M separate)
2. Trend: 3-month change rate
3. Ratio: Recent/historical comparison
4. Interaction: RF combined score
5. Flag: New donor indicator

**Deliverable:** Documented feature catalog

**Evaluation:** Do features predict donation in next month?

**Hint:** Start simple, validate early, iterate!

---

## Thank You!

**Key Takeaways:**

 Features are **stories** about donor behavior

 **Timeline compliance** is non-negotiable

 **RFM** + **Trends** + **Context** = powerful predictions

 **Document** and **validate** everything

 **Production** requires robust pipelines


# Part 1: Creating Dummy Variables

## The Problem: Categorical Variables

**Machine learning models need numbers, not categories!**

Consider this donor dataset:

| donor_id | gender | country | segment |
|----------|--------|---------|---------|
| 5 | F | India | Gold |
| 3 | M | USA | Silver |
| 2 | M | India | Bronze |
| 8 | F | UK | Silver |
| 1 | F | USA | Bronze |

**Question:** How do we include gender, country, and segment in a logistic regression model?

**Answer:** Convert categories to binary dummy variables (one-hot encoding)

---

## One-Hot Encoding: Gender Example

Transform each category into a binary indicator:

| donor_id | gender | gender_F | gender_M |
|----------|--------|----------|----------|
| 5 | F | 1 | 0 |
| 3 | M | 0 | 1 |
| 2 | M | 0 | 1 |
| 8 | F | 1 | 0 |
| 1 | F | 1 | 0 |

**Interpretation:** Each dummy variable answers a yes/no question. `gender_F = 1` means "Is this donor female? Yes."

---

## The Multicollinearity Problem

**Issue:** If we know `gender_F`, we automatically know `gender_M`!

When `gender_F = 0`, then `gender_M` must equal 1. This creates **perfect multicollinearity**, which breaks regression models.

**Mathematical problem:** gender_F + gender_M = 1 (always)

This linear dependence means the design matrix is not full rank, causing estimation failure.

**Solution:** Drop one category to serve as the reference level.

---

## Solution: Drop One Category

Keep only k-1 dummy variables for a categorical variable with k categories:

| donor_id | gender | gender_F |
|----------|--------|----------|
| 5 | F | 1 |
| 3 | M | 0 |
| 2 | M | 0 |
| 8 | F | 1 |
| 1 | F | 1 |

**Interpretation:** `gender_F = 0` implicitly means Male (reference category). The coefficient on `gender_F` represents the difference between Female and Male donors.

---

## Multi-Category Example: Country

For a variable with 3 categories, create 2 dummy variables:

| donor_id | country | country_USA | country_India |
|----------|---------|-------------|---------------|
| 5 | India | 0 | 1 |
| 3 | USA | 1 | 0 |
| 2 | India | 0 | 1 |
| 8 | UK | 0 | 0 |
| 1 | USA | 1 | 0 |

**Reference category:** UK (when both dummies equal 0).

**Model interpretation:** Coefficients measure effects relative to UK.

---

## Creating Dummy Variables in R
```{r, eval=FALSE}
# Method 1: Using fastDummies package
library(fastDummies)

# Create dummies, drop first category
basetable <- dummy_cols(
  basetable, 
  select_columns = "segment",
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)

# Method 2: Using model.matrix (automatic)
dummies <- model.matrix(~ segment - 1, data = basetable)

# Method 3: Manual approach for single variable
basetable$segment_Gold <- as.integer(basetable$segment == "Gold")
basetable$segment_Silver <- as.integer(basetable$segment == "Silver")
# Bronze becomes reference (both dummies = 0)
```

---

## Practical Example: Complete Pipeline
```{r, eval=FALSE}
# Original data
head(basetable[, c("donor_id", "segment")])
#   donor_id segment
# 1    32770    Gold
# 2    32776  Silver
# 3    32777  Bronze
# 4    65552  Bronze

# Create dummy variables
basetable <- dummy_cols(
  basetable,
  select_columns = "segment",
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)

# Result: Bronze is reference (omitted)
head(basetable[, c("donor_id", "segment_Gold", "segment_Silver")])
#   donor_id segment_Gold segment_Silver
# 1    32770            1              0
# 2    32776            0              1
# 3    32777            0              0
# 4    65552            0              0
```

---

# Part 2: Handling Missing Values

## Why Missing Values Matter

**Models cannot handle NA values!** Most algorithms will fail or silently drop observations.
```{r, eval=FALSE}
# Example: Missing age values
basetable[c("donor_id", "age")]
#   donor_id age
# 1        5  NA
# 2        3  25
# 3        2  36
# 4        8  40
# 5        1  26
```

**Critical questions:**

1. Why is the value missing? (Random? Systematic?)
2. How many values are missing? (1%? 50%?)
3. What's the best replacement strategy?

---

## Strategy 1: Replace with Mean

**When to use:** Continuous variables, few missing values, roughly normal distribution.
```{r, eval=FALSE}
# Calculate mean age (excluding NA)
mean_age <- mean(basetable$age, na.rm = TRUE)  # 31.75

# Replace missing values
basetable$age[is.na(basetable$age)] <- mean_age

# Result
#   donor_id age
# 1        5  31.75  # Was NA
# 2        3  25.00
# 3        2  36.00
# 4        8  40.00
# 5        1  26.00
```

**Advantage:** Simple, maintains overall mean.

**Disadvantage:** Reduces variance, breaks correlations.

---

## Strategy 2: Replace with Median (Better for Outliers)

**When to use:** Skewed distributions or presence of outliers.
```{r, eval=FALSE}
# Example with outlier in max_donation
basetable$max_donation
# 100, 1000000, 100, 40, 120

# Mean heavily influenced by outlier
mean(basetable$max_donation, na.rm = TRUE)    # 200,052
median(basetable$max_donation, na.rm = TRUE)  # 110

# Use median for robustness
replacement <- median(basetable$max_donation, na.rm = TRUE)
basetable$max_donation[is.na(basetable$max_donation)] <- replacement

# Result: Missing value now 110 (much more reasonable!)
```

**Why median?** The €1,000,000 donor skews the mean but doesn't affect the median.

---

## Strategy 3: Replace with Fixed Value (Zero)

**When to use:** Missing means "absence of activity" or "zero count."
```{r, eval=FALSE}
# Example: donations_last_year
# NA means they didn't donate (zero donations)
basetable[c("donor_id", "donations_last_year")]
#   donor_id donations_last_year
# 1        5                 130
# 2        3                  10
# 3        2                  NA  # No donations
# 4        8                  40
# 5        1                 120

# Replace NA with 0 (no donations)
basetable$donations_last_year[
  is.na(basetable$donations_last_year)
] <- 0

# Result: NA becomes 0 (meaningful zero)
```

**Semantic meaning:** Zero is not arbitrary here, it represents actual absence.

---

## General Missing Value Replacement Function
```{r, eval=FALSE}
# Flexible function for different strategies
replace_missing <- function(x, method = "mean") {
  if (method == "mean") {
    replacement <- mean(x, na.rm = TRUE)
  } else if (method == "median") {
    replacement <- median(x, na.rm = TRUE)
  } else if (method == "zero") {
    replacement <- 0
  } else {
    stop("Method must be 'mean', 'median', or 'zero'")
  }
  
  x[is.na(x)] <- replacement
  return(x)
}

# Usage
basetable$age <- replace_missing(basetable$age, method = "mean")
basetable$max_donation <- replace_missing(
  basetable$max_donation, 
  method = "median"
)
basetable$donations_last_year <- replace_missing(
  basetable$donations_last_year, 
  method = "zero"
)
```

---

## Strategy 4: Missing Value Indicator

**Concept:** Sometimes "missingness" itself is informative!
```{r, eval=FALSE}
# Example: email address
basetable[c("donor_id", "email")]
#   donor_id email
# 1    32770 person32770@provider.com
# 2    32776 NA
# 3    32777 person32777@provider.com
# 4    65552 NA

# Create indicator: 1 if missing, 0 if present
basetable$no_email <- as.integer(is.na(basetable$email))

# Result
#   donor_id email                    no_email
# 1    32770 person32770@provider.com        0
# 2    32776 NA                              1
# 3    32777 person32777@provider.com        0
# 4    65552 NA                              1
```

**Why this matters:** Missing email might indicate lower engagement or privacy concerns, which could be predictive!

---

## Missing Value Indicators in Practice

**Use cases where missingness is predictive:**

1. **Missing email** → Lower tech-savviness, privacy concerns
2. **Missing income** → Reluctance to share financial info
3. **Missing phone number** → Reduced contactability
4. **Missing survey responses** → Lower engagement
```{r, eval=FALSE}
# Create multiple missingness indicators
basetable$missing_email <- as.integer(is.na(basetable$email))
basetable$missing_phone <- as.integer(is.na(basetable$phone))
basetable$missing_income <- as.integer(is.na(basetable$income))

# These can be powerful predictors in the model!
# Example: Donors without email might be 30% less likely to donate
```

**Best practice:** Create indicator BEFORE imputing the actual variable.

---

# Part 3: Handling Outliers

## The Outlier Problem

**Outliers distort model coefficients and predictions.**

Consider this visualization:

\begin{center}
\begin{tikzpicture}[scale=0.8]
% Axes
\draw[->] (0,0) -- (10,0) node[right] {Age};
\draw[->] (0,0) -- (0,5) node[above] {P(Donate)};

% Most data points
\foreach \x in {2,2.5,3,3.5,4,4.5,5,5.5,6} {
  \fill[datablue] (\x,0.2) circle (2pt);
}

% Outlier
\fill[datared] (9.5,0.2) circle (3pt);

% Regression lines
\draw[datablue, thick] (1,1) -- (7,3.5);  % Without outlier
\draw[datared, thick, dashed] (1,0.5) -- (10,3);  % With outlier

\node[datablue] at (4,4) {Without outlier};
\node[datared] at (7,2) {With outlier};
\end{tikzpicture}
\end{center}

**Impact:** One extreme age value pulls the entire regression line!

---

## Causes of Outliers

**Before removing outliers, understand why they exist:**

1. **Data entry errors** → Age = 999 (should be 99)
2. **Measurement errors** → Equipment malfunction
3. **Truly extreme values** → Billionaire donor (real, but rare)
4. **Different population** → Corporate donor in individual database

**Decision tree:**

- Error? → Correct or remove
- Measurement issue? → Remove
- True extreme? → Cap (winsorize) or transform
- Different population? → Separate analysis or remove

---

## Method 1: Winsorization

**Concept:** Cap extreme values at specified percentiles (typically 5th and 95th).

\begin{center}
\begin{tikzpicture}[scale=0.7]
% Draw normal curve
\draw[thick] plot[smooth,domain=-3:3] (\x+5,{3*exp(-\x*\x/2)});
\fill[datagreen,opacity=0.3] plot[smooth,domain=-1.64:1.64] 
  (\x+5,{3*exp(-\x*\x/2)}) -- (6.64,0) -- (3.36,0) -- cycle;

% Markers
\draw[thick,datared] (3.36,0) -- (3.36,3) node[above] {5\%};
\draw[thick,datared] (6.64,0) -- (6.64,3) node[above] {95\%};

% Labels
\node at (2,0) [below] {EUR 6};
\node at (8,0) [below] {EUR 950};
\node at (5,4) {90\% of data unchanged};

\draw[<->,thick] (3.36,-0.5) -- (2,-0.5);
\draw[<->,thick] (6.64,-0.5) -- (8,-0.5);
\node at (2.5,-1) {Capped};
\node at (7.5,-1) {Capped};
\end{tikzpicture}
\end{center}

**Result:** Values below 5th percentile set to 5th percentile value. Values above 95th percentile set to 95th percentile value.

---

## Winsorization in R
```{r, eval=FALSE}
# Using DescTools package
library(DescTools)

# Winsorize at 5% and 95% percentiles
basetable$mean_donation_winsorized <- Winsorize(
  basetable$mean_donation,
  probs = c(0.05, 0.95)
)

# Manual implementation
winsorize_manual <- function(x, lower = 0.05, upper = 0.95) {
  # Calculate percentile thresholds
  lower_limit <- quantile(x, lower, na.rm = TRUE)
  upper_limit <- quantile(x, upper, na.rm = TRUE)
  
  # Cap values
  x[x < lower_limit] <- lower_limit
  x[x > upper_limit] <- upper_limit
  
  return(x)
}

# Apply manual function
basetable$mean_donation_winsorized <- winsorize_manual(
  basetable$mean_donation,
  lower = 0.05,
  upper = 0.95
)
```

---

## Method 2: Standard Deviation Method

**Concept:** Cap values beyond mean $\pm$ 3 standard deviations.

\begin{center}
\begin{tikzpicture}[scale=0.7]
% Draw normal curve
\draw[thick] plot[smooth,domain=-3.5:3.5] (\x+5,{3*exp(-\x*\x/2)});
\fill[datagreen,opacity=0.3] plot[smooth,domain=-3:3] 
  (\x+5,{3*exp(-\x*\x/2)}) -- (8,0) -- (2,0) -- cycle;

% Markers
\draw[thick,dashed] (2,0) -- (2,3.5);
\draw[thick,dashed] (8,0) -- (8,3.5);

% Labels
\node at (2,0) [below] {16};
\node at (5,0) [below] {54 (mean)};
\node at (8,0) [below] {130};
\node at (5,4) {99.7\% within 3 SD};

\node at (2,3.8) {$\mu - 3\sigma$};
\node at (8,3.8) {$\mu + 3\sigma$};
\end{tikzpicture}
\end{center}

**Why 3 SD?** In a normal distribution, 99.7% of data falls within this range. Values beyond are statistically extreme.

---

## Standard Deviation Method in R
```{r, eval=FALSE}
# Calculate boundaries
mean_age <- mean(basetable$age, na.rm = TRUE)
sd_age <- sd(basetable$age, na.rm = TRUE)

lower_limit <- mean_age - 3 * sd_age
upper_limit <- mean_age + 3 * sd_age

# Apply capping
basetable$age_no_outliers <- pmin(
  pmax(basetable$age, lower_limit),  # Cap below
  upper_limit                        # Cap above
)

# Alternative using ifelse
basetable$age_no_outliers <- ifelse(
  basetable$age < lower_limit, lower_limit,
  ifelse(basetable$age > upper_limit, upper_limit, 
         basetable$age)
)

# Function version
cap_outliers <- function(x, n_sd = 3) {
  mean_x <- mean(x, na.rm = TRUE)
  sd_x <- sd(x, na.rm = TRUE)
  lower <- mean_x - n_sd * sd_x
  upper <- mean_x + n_sd * sd_x
  pmin(pmax(x, lower), upper)
}
```

---

## Winsorization vs. Standard Deviation Method

**Comparison:**

| Method | When to Use | Advantage | Disadvantage |
|--------|-------------|-----------|--------------|
| **Winsorization** | Skewed data, many outliers | Preserves distribution shape | Arbitrary percentile choice |
| **Standard Deviation** | Normal data, few outliers | Statistically principled | Assumes normality |

**Example decision:**

- **Age variable** (roughly normal) → Use SD method
- **Donation amounts** (heavily skewed) → Use winsorization
```{r, eval=FALSE}
# Check distribution before choosing
hist(basetable$age)           # Normal? Use SD method
hist(basetable$mean_donation) # Skewed? Use winsorization

# Formal test for normality
shapiro.test(basetable$age)  # p > 0.05 suggests normality
```

---

# Part 4: Transformations

## Why Transform Variables?

**Problem:** Large differences in scale distort model predictions.

**Example:** Four donors with very different donation amounts:

- **Alice:** €100 (modest donor)
- **Bob:** €1,100 (10× more than Alice)
- **Carol:** €10,000 (100× more than Alice)
- **Dave:** €11,000 (only 10% more than Carol)

**Issue:** The difference between Alice and Bob (€1,000) seems similar to the difference between Carol and Dave (€1,000), but the relative importance is very different!

**Solution:** Transform to capture relative rather than absolute differences.

---

## Log Transformation: The Concept

**Logarithm converts multiplication into addition:**

- Alice: €100 → log(100) = 4.6
- Bob: €1,100 → log(1,100) = 7.0
- Carol: €10,000 → log(10,000) = 9.2
- Dave: €11,000 → log(11,000) = 9.3

**Key insight:** The gap between Alice and Bob (7.0 - 4.6 = 2.4) is now comparable to the gap between Carol and Dave (9.3 - 9.2 = 0.1). This better reflects the relative differences in giving capacity!

---

## Log Transformation Formula

**Mathematical properties:**

$$\log(a \times b) = \log(a) + \log(b)$$

This means percentage changes become additive:

- 10% increase: $\log(1.1 \times x) = \log(x) + 0.095$
- Same for all values of $x$!

\begin{center}
\begin{tikzpicture}[scale=0.6]
\draw[->] (0,0) -- (11,0) node[right] {Original value};
\draw[->] (0,0) -- (0,6) node[above] {Log value};
\draw[thick, datablue] plot[smooth,domain=0.5:10] (\x,{ln(\x)+2});
\node at (6,5) {Compresses large values};
\node at (6,4.5) {Expands small values};
\end{tikzpicture}
\end{center}

---

## Log Transformation in R
```{r, eval=FALSE}
# Apply natural logarithm
basetable$log_mean_donation <- log(basetable$mean_donation)

# Handle zeros (log(0) is undefined!)
# Add small constant before logging
basetable$log_mean_donation <- log(basetable$mean_donation + 1)

# Alternative: log1p function (more numerically stable)
basetable$log_mean_donation <- log1p(basetable$mean_donation)

# Compare distributions
par(mfrow = c(1, 2))
hist(basetable$mean_donation, main = "Original", 
     xlab = "Mean Donation")
hist(basetable$log_mean_donation, main = "Log-transformed",
     xlab = "Log(Mean Donation)")
```

**When to use:** Monetary values, counts, ratios, or any right-skewed variable.

---

## Other Useful Transformations

**Square root transformation** (milder than log):
```{r, eval=FALSE}
basetable$sqrt_donations <- sqrt(basetable$mean_donation)
```

**Inverse transformation** (for extreme skew):
```{r, eval=FALSE}
basetable$inv_recency <- 1 / (basetable$days_since_last + 1)
# Recent donors get high values, old donors get low values
```

**Box-Cox transformation** (automatically finds best power):
```{r, eval=FALSE}
library(car)
bc <- powerTransform(basetable$mean_donation)
basetable$bc_donation <- bcPower(basetable$mean_donation, 
                                  bc$lambda)
```

---

# Part 5: Interaction Features

## The Concept of Interactions

**Problem:** Features may have combined effects that are greater than the sum of their parts.

**Example:** Donor engagement depends on BOTH frequency and recency:

- **High frequency + Recent donation** → Very likely to donate
- **High frequency + Old donation** → Moderate likelihood
- **Low frequency + Recent donation** → Moderate likelihood
- **Low frequency + Old donation** → Very unlikely to donate

The effect of frequency depends on recency (and vice versa)!

---

## Interaction Visualization

\begin{center}
\begin{tikzpicture}[scale=0.9]
% Timeline arrows
\draw[->,ultra thick,datagreen] (0,3) -- (9,3);
\draw[->,ultra thick,datared] (0,0.5) -- (9,0.5);

% Donors on first timeline (high frequency)
\foreach \x in {0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5} {
  \fill[datagreen] (\x,3) circle (3pt);
}

% Donors on second timeline (low frequency)
\fill[datared] (0.7,0.5) circle (3pt);
\fill[datared] (7,0.5) circle (3pt);

% Labels
\node at (-1.5,3) [left] {High frequency:};
\node at (-1.5,0.5) [left] {Low frequency:};
\node at (9.5,3) [right] {Likely};
\node at (9.5,0.5) [right] {Unlikely};
\node at (4.5,-0.5) {\textbf{Recency: Recent $\rightarrow$ Old}};
\end{tikzpicture}
\end{center}

**Key insight:** The combination of high frequency AND recent donation creates the strongest prediction signal.

---

## Creating Interactions in R
```{r, eval=FALSE}
# Multiplicative interaction
basetable$freq_recency_interaction <- 
  basetable$donation_count * basetable$days_since_last

# Interpretation: Higher values = active donors
# Recent + frequent donors get high scores
# Old + infrequent donors get low scores

# Alternative: Inverse recency for intuition
basetable$freq_recency_interaction <- 
  basetable$donation_count / (basetable$days_since_last + 1)

# Now higher values = more engaged
# Recent frequent donors: 12 / 10 = 1.2
# Old infrequent donors: 2 / 365 = 0.005

# Multiple interactions
basetable$rfm_interaction <- 
  (1 / (basetable$days_since_last + 1)) *  # Recency
  basetable$donation_count *                # Frequency
  basetable$mean_donation                   # Monetary
```

---

## Interaction Example: RFM Combined Score
```{r, eval=FALSE}
# Create comprehensive engagement score
compute_rfm_score <- function(recency, frequency, monetary) {
  # Normalize recency (inverse so recent = high)
  r_score <- 1 / (recency + 1)
  
  # Normalize frequency (already in right direction)
  f_score <- frequency
  
  # Normalize monetary
  m_score <- monetary
  
  # Combined multiplicative score
  rfm_score <- r_score * f_score * m_score
  
  return(rfm_score)
}

# Apply to all donors
basetable$rfm_score <- compute_rfm_score(
  basetable$days_since_last,
  basetable$donation_count,
  basetable$mean_donation
)

# Top 10 most engaged donors
head(basetable[order(-basetable$rfm_score), 
               c("donor_id", "rfm_score")], 10)
```

---

## Automatic Interaction Creation
```{r, eval=FALSE}
# Create interactions between all numeric variables
numeric_vars <- c("days_since_last", "donation_count", 
                  "mean_donation", "max_donation")

# All pairwise interactions
for (i in 1:(length(numeric_vars)-1)) {
  for (j in (i+1):length(numeric_vars)) {
    var1 <- numeric_vars[i]
    var2 <- numeric_vars[j]
    
    interaction_name <- paste0(var1, "_x_", var2)
    basetable[[interaction_name]] <- basetable[[var1]] * 
                                      basetable[[var2]]
  }
}

# Result: Creates
# days_since_last_x_donation_count
# days_since_last_x_mean_donation
# donation_count_x_mean_donation
# ... and more

# Warning: Can create many features quickly!
```

---

# Part 6: Complete Preprocessing Pipeline

## Putting It All Together
```{r, eval=FALSE}
# Step 1: Handle missing values
basetable$age[is.na(basetable$age)] <- 
  median(basetable$age, na.rm = TRUE)

basetable$donations_last_year[is.na(basetable$donations_last_year)] <- 0

basetable$missing_email <- as.integer(is.na(basetable$email))

# Step 2: Handle outliers
basetable$mean_donation_capped <- cap_outliers(
  basetable$mean_donation, 
  n_sd = 3
)

# Step 3: Create dummy variables
basetable <- dummy_cols(
  basetable,
  select_columns = c("segment", "country"),
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)

# Step 4: Transform skewed variables
basetable$log_mean_donation <- log1p(basetable$mean_donation_capped)

# Step 5: Create interactions
basetable$rfm_score <- compute_rfm_score(
  basetable$days_since_last,
  basetable$donation_count,
  basetable$mean_donation
)
```

---

## Preprocessing Checklist

Before modeling, verify:

- Missing values handled for ALL variables
- Outliers addressed in key variables
- Categorical variables converted to dummies
- Skewed variables transformed
- Meaningful interactions created
- No infinite or undefined values (check log(0))
- Feature names are clear and documented
- Original variables retained (for interpretation)
```{r, eval=FALSE}
# Final data quality check
summary(basetable)  # Check for NA, Inf
sapply(basetable, function(x) sum(is.na(x)))  # Count NAs
sapply(basetable, function(x) sum(is.infinite(x)))  # Count Inf
```

---

## Feature Naming Convention

**Good practice:** Use clear, consistent names.
```{r, eval=FALSE}
# Bad naming
basetable$var1 <- ...
basetable$x_new <- ...
basetable$temp2 <- ...

# Good naming
basetable$age_median_imputed <- ...
basetable$mean_donation_log <- ...
basetable$rfm_interaction_score <- ...

# Pattern: [variable]_[transformation]_[method]
# Examples:
# - age_winsorized_95
# - donation_log_plus1
# - recency_inverse
# - freq_rec_interaction
# - segment_Gold_dummy
```

**Why this matters:** In 6 months, you'll thank yourself!

---

## Summary: Preprocessing Workflow

\begin{center}
\begin{tikzpicture}[scale=0.8,
  box/.style={rectangle, draw, thick, text width=3cm, align=center, 
              minimum height=1cm, fill=datablue!20}]

\node[box] (raw) at (0,0) {Raw Data};
\node[box] (missing) at (4,0) {Handle Missing};
\node[box] (outliers) at (8,0) {Handle Outliers};
\node[box] (dummy) at (0,-2) {Create Dummies};
\node[box] (transform) at (4,-2) {Transform};
\node[box] (interact) at (8,-2) {Interactions};
\node[box] (ready) at (4,-4) {Model-Ready Data};

\draw[->,thick] (raw) -- (missing);
\draw[->,thick] (missing) -- (outliers);
\draw[->,thick] (outliers) -- (dummy);
\draw[->,thick] (dummy) -- (transform);
\draw[->,thick] (transform) -- (interact);
\draw[->,thick] (interact) -- (ready);

\end{tikzpicture}
\end{center}

Each step prepares data for better model performance!

---

## Common Pitfalls to Avoid

**Mistake 1:** Forgetting to drop one dummy category

- Creates multicollinearity
- Model fails to converge

**Mistake 2:** Taking log of zero

- Returns -Inf
- Use `log1p()` or add small constant

**Mistake 3:** Not documenting imputation choices

- Can't reproduce results
- Don't remember why you chose median vs. mean

**Mistake 4:** Creating too many interactions

- 10 variables → 45 pairwise interactions!
- Use domain knowledge to select meaningful ones

**Mistake 5:** Transforming the target variable

- Makes interpretation difficult
- Only transform predictors, not the outcome

---

## Next Steps: Ready for Modeling

With preprocessed features, you can now:

1. **Split data** into training and testing sets
2. **Build models** using clean, numeric features
3. **Evaluate performance** without data quality issues
4. **Interpret results** using meaningful transformations

**Coming up next:**

- Model selection and training
- Cross-validation strategies
- Performance evaluation metrics
- Model interpretation and deployment

\vspace{2em}

\begin{center}
\Large{\textcolor{datablue}{\textbf{Questions?}}}
\end{center}



# Part 7: Scaling and Normalization

## Why Scale Features?

**Problem:** Different features have vastly different scales.

Consider a logistic regression model predicting donations:

| Feature | Range | Coefficient | Scaled Impact |
|---------|-------|-------------|---------------|
| Age | 18-90 years | 0.02 | 0.02 × 72 = 1.44 |
| Income | €20,000-€200,000 | 0.00001 | 0.00001 × 180,000 = 1.8 |
| Days since last gift | 1-3,650 days | 0.001 | 0.001 × 3,649 = 3.65 |

**Issue:** The coefficient magnitudes don't reflect true importance!

**Solution:** Standardize all features to comparable scales.

## Method 1: Min-Max Scaling

**Concept:** Transform values to range [0, 1].

$$x_{\text{scaled}} = \frac{x - \min(x)}{\max(x) - \min(x)}$$
```{r eval=FALSE}
# Min-max scaling function
min_max_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / 
    (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Apply to age variable
basetable$age_scaled <- min_max_scale(basetable$age)

# Result: All values between 0 and 1
# Original: 18, 25, 45, 90
# Scaled:   0.00, 0.10, 0.38, 1.00
```

**Advantage:** Bounded output, preserves zero.

**Disadvantage:** Sensitive to outliers.

## Min-Max Scaling Example
```{r}
# Example data
age_original <- c(18, 25, 35, 45, 60, 90)
age_scaled <- (age_original - min(age_original)) / 
              (max(age_original) - min(age_original))

example_data <- tibble(
  donor_id = 1:6,
  age_original = age_original,
  age_scaled = round(age_scaled, 3)
)

kable(example_data)
```

**Interpretation:** Donor 1 (age 18) gets 0.000, Donor 6 (age 90) gets 1.000.

## Method 2: Standardization (Z-score)

**Concept:** Transform to mean = 0, standard deviation = 1.

$$x_{\text{standardized}} = \frac{x - \mu}{\sigma}$$
```{r eval=FALSE}
# Standardization function
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Apply to donation amounts
basetable$mean_donation_std <- standardize(basetable$mean_donation)

# Result: Values centered at 0
# Original: 50, 100, 150, 200
# Std:     -1.16, -0.39, 0.39, 1.16
```

**Advantage:** Not bounded, interpretable units.

**Disadvantage:** No fixed range.

## Standardization Example
```{r}
# Example data
donation <- c(50, 100, 150, 200, 250, 1000)
donation_std <- (donation - mean(donation)) / sd(donation)

std_example <- tibble(
  donor_id = 1:6,
  donation_original = donation,
  donation_std = round(donation_std, 2)
)

kable(std_example)
```

**Interpretation:** Donor 6 (outlier) is 2.20 standard deviations above mean.

## Min-Max vs. Standardization: When to Use

**Use Min-Max Scaling when:**

- Need bounded output [0, 1]
- Working with neural networks
- Features have known, fixed ranges
- Preserving exact zero is important

**Use Standardization when:**

- Working with SVM, logistic regression
- Data contains outliers
- Want interpretable units (standard deviations)
- Using regularization (L1/L2)

**Rule of thumb:** Standardization is default for most models.

## Scaling Multiple Features in R
```{r eval=FALSE}
# Identify numeric columns to scale
numeric_cols <- c("age", "mean_donation", "donation_count", 
                  "days_since_last")

# Method 1: Using base R scale()
basetable[paste0(numeric_cols, "_std")] <- 
  scale(basetable[numeric_cols])

# Method 2: Using dplyr
basetable <- basetable %>%
  mutate(across(
    all_of(numeric_cols),
    ~scale(.)[,1],
    .names = "{.col}_std"
  ))

# Method 3: Using recipes package
library(recipes)

scaling_recipe <- recipe(target ~ ., data = basetable) %>%
  step_normalize(all_numeric(), -all_outcomes())

prepped_recipe <- prep(scaling_recipe, training = basetable)
basetable_scaled <- bake(prepped_recipe, new_data = basetable)
```

## Critical: Save Scaling Parameters!

**Problem:** Must use SAME scaling parameters for test data.
```{r eval=FALSE}
# WRONG: Scale test data independently
test_scaled_wrong <- scale(test_data)  # Uses test mean/sd!

# RIGHT: Use training parameters
train_mean <- mean(train_data$age, na.rm = TRUE)
train_sd <- sd(train_data$age, na.rm = TRUE)

test_data$age_std <- (test_data$age - train_mean) / train_sd

# Better: Save parameters explicitly
scaling_params <- list(
  age_mean = mean(basetable$age, na.rm = TRUE),
  age_sd = sd(basetable$age, na.rm = TRUE),
  donation_mean = mean(basetable$mean_donation, na.rm = TRUE),
  donation_sd = sd(basetable$mean_donation, na.rm = TRUE)
)

saveRDS(scaling_params, "scaling_parameters.rds")
```

**Why this matters:** Test data scaled with test statistics leaks information!

## Robust Scaling: Handling Outliers

**Problem:** Standard scaling uses mean/sd (sensitive to outliers).

**Solution:** Use median and IQR instead.

$$x_{\text{robust}} = \frac{x - \text{median}(x)}{\text{IQR}(x)}$$
```{r eval=FALSE}
# Robust scaling function
robust_scale <- function(x) {
  med <- median(x, na.rm = TRUE)
  iqr <- IQR(x, na.rm = TRUE)
  (x - med) / iqr
}

# Apply to donation amounts
basetable$mean_donation_robust <- robust_scale(
  basetable$mean_donation
)

# Compare to standard scaling
basetable$mean_donation_std <- standardize(
  basetable$mean_donation
)
```

**When to use:** Extreme outliers you want to preserve (not cap).

## Scaling Visualization

\begin{center}
\begin{tikzpicture}[scale=0.8]
% Original data
\draw[->] (0,0) -- (10,0) node[right] {Original};
\foreach \x in {1,2,8,9} {
  \fill[datablue] (\x,0) circle (2pt);
}
\fill[datared] (9.5,0) circle (3pt);
\node at (0,0) [below] {10};
\node at (10,0) [below] {1000};

% Min-max scaled
\draw[->] (0,-1.5) -- (10,-1.5) node[right] {Min-Max};
\foreach \x/\val in {0.1/0, 0.2/0.1, 0.8/0.8, 0.9/0.9, 1.0/1.0} {
  \fill[datablue] (\x*10,-1.5) circle (2pt);
}
\node at (0,-1.5) [below] {0.0};
\node at (10,-1.5) [below] {1.0};

% Standardized
\draw[->] (0,-3) -- (10,-3) node[right] {Standardized};
\foreach \x/\val in {3/-1.5, 3.5/-1.2, 6/0, 6.5/0.3, 7/2.5} {
  \fill[datablue] (\x,-3) circle (2pt);
}
\node at (0,-3) [below] {$-2\sigma$};
\node at (5,-3) [below] {$0$};
\node at (10,-3) [below] {$+2\sigma$};
\end{tikzpicture}
\end{center}

# Part 8: Feature Engineering Best Practices

## The Feature Engineering Checklist

Before finalizing your feature set:

\begin{enumerate}
\item \textcolor{datablue}{\textbf{Timeline compliance}} \\
  $\square$ All features use only past data \\
  $\square$ No data leakage from target period

\item \textcolor{datablue}{\textbf{Data quality}} \\
  $\square$ Missing values handled appropriately \\
  $\square$ Outliers addressed or documented \\
  $\square$ No infinite or undefined values

\item \textcolor{datablue}{\textbf{Encoding}} \\
  $\square$ Categorical variables converted to dummies \\
  $\square$ Reference categories documented

\item \textcolor{datablue}{\textbf{Transformations}} \\
  $\square$ Skewed variables transformed \\
  $\square$ Variables scaled appropriately
\end{enumerate}

## Feature Engineering Checklist (cont.)

\begin{enumerate}
\setcounter{enumi}{4}
\item \textcolor{datablue}{\textbf{Feature creation}} \\
  $\square$ Interaction terms included \\
  $\square$ Domain-specific features created \\
  $\square$ Temporal patterns captured

\item \textcolor{datablue}{\textbf{Documentation}} \\
  $\square$ Feature catalog created \\
  $\square$ Calculation formulas documented \\
  $\square$ Scaling parameters saved

\item \textcolor{datablue}{\textbf{Validation}} \\
  $\square$ Distributions examined \\
  $\square$ Correlations checked \\
  $\square$ Feature importance assessed
\end{enumerate}

## Common Feature Engineering Mistakes

**Mistake 1: Data Leakage**
```{r eval=FALSE}
# WRONG: Using target period data
basetable$donations_total <- sum_all_donations()  # Includes future!

# RIGHT: Use only historical data
basetable$donations_historical <- sum_donations_before(obs_date)
```

**Mistake 2: Forgetting to Save Parameters**
```{r eval=FALSE}
# WRONG: No way to reproduce scaling
train_scaled <- scale(train_data)

# RIGHT: Save and reuse parameters
scaling_params <- list(mean = mean(train_data$age), 
                      sd = sd(train_data$age))
saveRDS(scaling_params, "params.rds")
```

**Mistake 3: Overfitting on Training Data**
```{r eval=FALSE}
# WRONG: 100 interaction terms, 50 observations
basetable <- create_all_possible_interactions()  # Overfitting!

# RIGHT: Select meaningful interactions
basetable$key_interaction <- recency * frequency
```

## Feature Engineering Anti-Patterns

**Anti-pattern 1: "Let's add everything!"**

- Creates overfitting
- Makes model uninterpretable
- Increases computational cost

**Anti-pattern 2: "Let's drop everything with missing values!"**

- Loses valuable information
- Reduces sample size unnecessarily
- May introduce bias

**Anti-pattern 3: "Let's use the same features for every problem!"**

- Ignores domain specifics
- Misses important signals
- Reduces model performance

## Feature Quality Metrics
```{r eval=FALSE}
# 1. Correlation with target
correlations <- cor(basetable[numeric_cols], 
                    basetable$target, 
                    use = "complete.obs")
print(sort(abs(correlations), decreasing = TRUE))

# 2. Information value (IV)
library(Information)
IV <- create_infotables(data = basetable, 
                        y = "target",
                        bins = 10)
print(IV$Summary)  # IV > 0.1 is useful

# 3. Variance inflation factor (VIF)
library(car)
vif_values <- vif(lm(target ~ ., data = basetable))
print(vif_values[vif_values > 5])  # VIF > 5 = collinearity
```

## Feature Correlation Matrix
```{r eval=FALSE}
library(corrplot)

# Select numeric features
numeric_features <- basetable %>% 
  select(where(is.numeric), -donor_id)

# Compute correlation matrix
cor_matrix <- cor(numeric_features, use = "complete.obs")

# Visualize
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.7)

# Flag high correlations (|r| > 0.9)
high_cor <- which(abs(cor_matrix) > 0.9 & cor_matrix != 1, 
                  arr.ind = TRUE)
```

# Part 9: Production Pipeline

## From Development to Production

**Development:** Ad-hoc feature engineering in notebook

**Production:** Reproducible, maintainable pipeline

\begin{center}
\begin{tikzpicture}[scale=0.7,
  box/.style={rectangle, draw, thick, text width=2.5cm, align=center, 
              minimum height=1cm}]

\node[box, fill=datared!20] (raw) at (0,0) {Raw Data};
\node[box, fill=datablue!20] (clean) at (3.5,0) {Clean Data};
\node[box, fill=datagreen!20] (features) at (7,0) {Features};
\node[box, fill=datapurple!20] (scaled) at (10.5,0) {Scaled};
\node[box, fill=datacyan!20] (model) at (10.5,-2) {Model};

\draw[->,thick] (raw) -- (clean) node[midway,above] {\tiny handle NA};
\draw[->,thick] (clean) -- (features) node[midway,above] {\tiny engineer};
\draw[->,thick] (features) -- (scaled) node[midway,above] {\tiny scale};
\draw[->,thick] (scaled) -- (model) node[midway,right] {\tiny predict};

\end{tikzpicture}
\end{center}

## Creating a Feature Engineering Function
```{r eval=FALSE}
engineer_features <- function(data, reference_date, params = NULL) {
  # Step 1: Handle missing values
  data <- data %>%
    mutate(
      age = replace_na(age, median(age, na.rm = TRUE)),
      donations_last_year = replace_na(donations_last_year, 0),
      missing_email = as.integer(is.na(email))
    )
  
  # Step 2: Create time-based features
  data <- data %>%
    mutate(
      days_since_last = as.numeric(reference_date - last_donation_date)
    )
  
  # Step 3: Create RFM features
  data <- data %>%
    mutate(
      rfm_score = (1 / (days_since_last + 1)) * 
                  donation_count * mean_donation
    )
  
  # Step 4: Create interactions
  data <- data %>%
    mutate(
      freq_recency = donation_count * (1 / (days_since_last + 1))
    )
  
  return(data)
}
```

## Parameterizing the Pipeline
```{r eval=FALSE}
# Save configuration
feature_config <- list(
  reference_date = as.Date("2024-01-01"),
  missing_strategy = list(
    age = "median",
    donations_last_year = "zero",
    email = "indicator"
  ),
  outlier_method = "winsorize",
  outlier_probs = c(0.05, 0.95),
  scaling_method = "standardize",
  dummy_variables = c("segment", "country"),
  interaction_terms = list(
    c("donation_count", "days_since_last"),
    c("mean_donation", "donation_count")
  )
)

# Save configuration
saveRDS(feature_config, "feature_config.rds")

# Use in pipeline
config <- readRDS("feature_config.rds")
basetable_features <- engineer_features(
  basetable, 
  reference_date = config$reference_date
)
```

## Building a Recipes Pipeline
```{r eval=FALSE}
library(recipes)

# Define preprocessing recipe
preprocessing_recipe <- recipe(target ~ ., data = basetable) %>%
  
  # Step 1: Remove ID variables
  step_rm(donor_id) %>%
  
  # Step 2: Create dummy variables
  step_dummy(all_nominal(), -all_outcomes(), 
             one_hot = FALSE) %>%
  
  # Step 3: Impute missing values
  step_impute_median(all_numeric(), -all_outcomes()) %>%
  
  # Step 4: Remove zero variance features
  step_zv(all_predictors()) %>%
  
  # Step 5: Normalize numeric features
  step_normalize(all_numeric(), -all_outcomes()) %>%
  
  # Step 6: Create interactions
  step_interact(~ days_since_last:donation_count) %>%
  
  # Step 7: Remove highly correlated features
  step_corr(all_numeric(), threshold = 0.9)
```

## Using the Recipe
```{r eval=FALSE}
# Prepare recipe on training data
prepped_recipe <- prep(preprocessing_recipe, 
                       training = train_data)

# Apply to training data
train_processed <- bake(prepped_recipe, 
                        new_data = train_data)

# Apply to test data (uses training parameters!)
test_processed <- bake(prepped_recipe, 
                       new_data = test_data)

# Apply to new scoring data
new_data_processed <- bake(prepped_recipe, 
                           new_data = new_donors)

# Save recipe for production
saveRDS(prepped_recipe, "preprocessing_recipe.rds")

# Load in production
recipe <- readRDS("preprocessing_recipe.rds")
scored_data <- bake(recipe, new_data = incoming_data)
```

## Unit Testing Your Features
```{r eval=FALSE}
library(testthat)

test_that("Timeline compliance is maintained", {
  # Create test data with known dates
  test_data <- tibble(
    donor_id = 1,
    donation_date = as.Date(c("2023-01-01", "2024-06-01")),
    amount = c(100, 50)
  )
  
  ref_date <- as.Date("2024-01-01")
  
  # Process features
  features <- engineer_features(test_data, ref_date)
  
  # Assert: Only Jan 2023 donation should count
  expect_equal(features$donations_historical[1], 100)
  expect_equal(features$donation_count[1], 1)
})

test_that("Missing value handling works correctly", {
  test_data <- tibble(age = c(25, NA, 35))
  
  # Process
  processed <- handle_missing(test_data, method = "median")
  
  # Assert: No NA values remain
  expect_true(all(!is.na(processed$age)))
  expect_equal(processed$age[2], 30)
})
```

## Monitoring Feature Drift
```{r eval=FALSE}
monitor_drift <- function(new_data, baseline_data) {
  
  numeric_features <- names(baseline_data)[
    sapply(baseline_data, is.numeric)
  ]
  
  drift_report <- map_df(numeric_features, function(feat) {
    
    # Kolmogorov-Smirnov test
    ks_result <- ks.test(baseline_data[[feat]], 
                         new_data[[feat]])
    
    # Calculate distribution statistics
    baseline_mean <- mean(baseline_data[[feat]], na.rm = TRUE)
    current_mean <- mean(new_data[[feat]], na.rm = TRUE)
    
    tibble(
      feature = feat,
      ks_statistic = ks_result$statistic,
      ks_pvalue = ks_result$p.value,
      drift_detected = ks_result$p.value < 0.05,
      baseline_mean = baseline_mean,
      current_mean = current_mean,
      mean_change_pct = (current_mean - baseline_mean) / 
                        baseline_mean * 100
    )
  })
  
  return(drift_report)
}
```

## Drift Detection Example
```{r eval=FALSE}
# Load baseline (training) data
baseline <- readRDS("training_data_jan2024.rds")

# New data from production
new_batch <- readRDS("production_data_nov2024.rds")

# Check for drift
drift_analysis <- monitor_drift(new_batch, baseline)

# Flag significant drifts
drift_analysis %>%
  filter(drift_detected == TRUE) %>%
  arrange(ks_pvalue) %>%
  select(feature, ks_statistic, ks_pvalue, mean_change_pct)
```

# Part 10: Advanced Topics

## Feature Selection Methods
```{r eval=FALSE}
# Method 1: Correlation-based
cor_with_target <- cor(basetable[numeric_cols], 
                        basetable$target)
top_features <- names(sort(abs(cor_with_target), 
                          decreasing = TRUE)[1:20])

# Method 2: Chi-square for categorical
library(FSelector)
chi_scores <- chi.squared(target ~ ., data = basetable)
top_categorical <- cutoff.k(chi_scores, k = 10)
```

## Recursive Feature Elimination
```{r eval=FALSE}
library(caret)

# Define control
ctrl <- rfeControl(
  functions = rfFuncs,
  method = "cv",
  number = 5
)

# Run RFE
rfe_results <- rfe(
  x = basetable %>% select(-target, -donor_id),
  y = basetable$target,
  sizes = c(5, 10, 15, 20, 25),
  rfeControl = ctrl
)

# Optimal features
print(rfe_results$optVariables)

# Plot
plot(rfe_results, type = c("g", "o"))
```

## LASSO Regularization
```{r eval=FALSE}
library(glmnet)

# Prepare matrix
X <- model.matrix(target ~ . - donor_id, data = basetable)[,-1]
y <- basetable$target

# Fit LASSO
lasso_model <- cv.glmnet(X, y, 
                         family = "binomial", 
                         alpha = 1,
                         nfolds = 5)

# Extract non-zero coefficients
lasso_coefs <- coef(lasso_model, s = "lambda.min")
selected_features <- rownames(lasso_coefs)[lasso_coefs[,1] != 0]

print(selected_features)

# Visualize
plot(lasso_model)
```

## Polynomial Features
```{r eval=FALSE}
# Create polynomial features
basetable <- basetable %>%
  mutate(
    # Quadratic terms
    age_squared = age^2,
    donation_squared = mean_donation^2,
    
    # Cubic terms
    age_cubed = age^3,
    
    # Square root
    age_sqrt = sqrt(age),
    
    # Ratio
    age_to_donation_ratio = age / (mean_donation + 1)
  )

# Automated polynomial creation
library(recipes)

poly_recipe <- recipe(target ~ ., data = basetable) %>%
  step_poly(age, degree = 2) %>%
  step_poly(mean_donation, degree = 2)

poly_data <- prep(poly_recipe) %>% bake(new_data = basetable)
```

## Time-Based Features
```{r eval=FALSE}
# Extract temporal patterns
basetable <- basetable %>%
  mutate(
    # Day of week
    donation_day_of_week = wday(last_donation_date),
    
    # Month
    donation_month = month(last_donation_date),
    
    # Quarter
    donation_quarter = quarter(last_donation_date),
    
    # Is weekend?
    is_weekend = wday(last_donation_date) %in% c(1, 7),
    
    # Is December?
    is_december = month(last_donation_date) == 12,
    
    # Days until year end
    days_to_year_end = as.numeric(
      ceiling_date(last_donation_date, "year") - 
      last_donation_date
    )
  )
```

## Lag Features
```{r eval=FALSE}
# Calculate donations by month
monthly_donations <- gifts %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(donor_id, month) %>%
  summarize(monthly_total = sum(amount), .groups = "drop")

# Create lag features
monthly_donations <- monthly_donations %>%
  group_by(donor_id) %>%
  arrange(month) %>%
  mutate(
    donation_lag1 = lag(monthly_total, 1),
    donation_lag2 = lag(monthly_total, 2),
    donation_lag3 = lag(monthly_total, 3),
    
    # Rolling average
    donation_ma3 = (donation_lag1 + donation_lag2 + 
                    donation_lag3) / 3,
    
    # Change from last month
    donation_change = monthly_total - donation_lag1,
    donation_change_pct = (monthly_total - donation_lag1) / 
                          (donation_lag1 + 1) * 100
  ) %>%
  ungroup()
```

## Target Encoding
```{r eval=FALSE}
# Calculate target rate by country
country_encoding <- basetable %>%
  group_by(country) %>%
  summarize(
    target_rate = mean(target, na.rm = TRUE),
    n = n()
  )

# Add smoothing
overall_rate <- mean(basetable$target, na.rm = TRUE)
smoothing_factor <- 10

country_encoding <- country_encoding %>%
  mutate(
    target_rate_smoothed = (target_rate * n + 
                           overall_rate * smoothing_factor) / 
                           (n + smoothing_factor)
  )

# Join back
basetable <- basetable %>%
  left_join(country_encoding %>% 
            select(country, target_rate_smoothed),
            by = "country")
```

## Feature Hashing
```{r eval=FALSE}
library(FeatureHashing)

# Hash country into buckets
hashed_features <- hashed.model.matrix(
  ~ country,
  data = basetable,
  hash.size = 2^10,  # 1024 buckets
  signed.hash = FALSE
)

# Add to basetable
basetable <- cbind(basetable, as.data.frame(hashed_features))
```

# Part 11: Complete Case Study

## Business Problem

**Organization:** International nonprofit

**Goal:** Predict €50+ donations in next 3 months

**Data:** 5 years history, 100,000 donors

**Timeline:** Build model January 2024

## Step 1: Define Timeline
```{r eval=FALSE}
# Observation date
observation_date <- as.Date("2024-01-01")

# Target period
target_start <- observation_date
target_end <- observation_date + months(3)

# Historical window
history_start <- observation_date - years(2)
history_end <- observation_date

# Partition data
gifts_historical <- gifts %>%
  filter(date >= history_start & date < history_end)

gifts_target <- gifts %>%
  filter(date >= target_start & date < target_end)
```

## Step 2: Define Population
```{r eval=FALSE}
# Eligible donors
donors_with_history <- gifts_historical %>%
  distinct(donor_id)

# Exclude buffer period
buffer_start <- observation_date - months(1)
buffer_donations <- gifts %>%
  filter(date >= buffer_start & date < observation_date) %>%
  distinct(donor_id)

# Final population
population <- setdiff(
  donors_with_history$donor_id,
  buffer_donations$donor_id
)

# Create basetable
basetable <- tibble(donor_id = population)
```

## Step 3: Calculate Target
```{r eval=FALSE}
# Aggregate target
target_donations <- gifts_target %>%
  group_by(donor_id) %>%
  summarize(target_amount = sum(amount), .groups = "drop")

# Join and create binary target
basetable <- basetable %>%
  left_join(target_donations, by = "donor_id") %>%
  mutate(
    target_amount = replace_na(target_amount, 0),
    target = as.integer(target_amount >= 50)
  )

# Check distribution
table(basetable$target)
```

## Step 4: Engineer Features - RFM
```{r eval=FALSE}
# RFM features
rfm_features <- gifts_historical %>%
  group_by(donor_id) %>%
  summarize(
    # Recency
    days_since_last = as.numeric(observation_date - max(date)),
    
    # Frequency
    donation_count = n(),
    unique_months = n_distinct(floor_date(date, "month")),
    
    # Monetary
    total_donated = sum(amount),
    mean_donation = mean(amount),
    median_donation = median(amount),
    max_donation = max(amount),
    min_donation = min(amount),
    cv_donation = sd(amount) / mean(amount),
    
    .groups = "drop"
  )

basetable <- basetable %>%
  left_join(rfm_features, by = "donor_id")
```

## Step 5: Trend Features
```{r eval=FALSE}
# Recent vs previous
recent_3m <- gifts_historical %>%
  filter(date >= observation_date - months(3)) %>%
  group_by(donor_id) %>%
  summarize(donations_3m = sum(amount), .groups = "drop")

previous_3m <- gifts_historical %>%
  filter(date >= observation_date - months(6),
         date < observation_date - months(3)) %>%
  group_by(donor_id) %>%
  summarize(donations_prev3m = sum(amount), .groups = "drop")

trend_features <- recent_3m %>%
  full_join(previous_3m, by = "donor_id") %>%
  mutate(
    donations_3m = replace_na(donations_3m, 0),
    donations_prev3m = replace_na(donations_prev3m, 0),
    trend_absolute = donations_3m - donations_prev3m,
    trend_percent = (donations_3m - donations_prev3m) / 
                    (donations_prev3m + 1) * 100
  )

basetable <- basetable %>%
  left_join(trend_features, by = "donor_id")
```

## Step 6: Demographics
```{r eval=FALSE}
# Load demographics
demographics <- read_csv("donor_demographics.csv")

# Calculate age
demographics <- demographics %>%
  mutate(
    age = year(observation_date) - year(birth_date),
    tenure_days = as.numeric(observation_date - first_donation_date)
  )

# Join
basetable <- basetable %>%
  left_join(
    demographics %>% select(donor_id, age, gender, 
                           country, tenure_days),
    by = "donor_id"
  )
```

## Step 7: Data Cleaning
```{r eval=FALSE}
# Handle missing values
basetable <- basetable %>%
  mutate(
    # Impute age
    age = replace_na(age, median(age, na.rm = TRUE)),
    
    # Zero for donations
    across(starts_with("donations_"), 
           ~replace_na(., 0)),
    
    # Missing indicators
    missing_gender = as.integer(is.na(gender)),
    missing_country = as.integer(is.na(country))
  )

# Handle outliers
basetable <- basetable %>%
  mutate(
    mean_donation_capped = pmin(mean_donation, 
                                quantile(mean_donation, 0.95, 
                                       na.rm = TRUE)),
    max_donation_capped = pmin(max_donation,
                              quantile(max_donation, 0.95, 
                                     na.rm = TRUE))
  )
```

## Step 8: Transformations
```{r eval=FALSE}
# Log transformations
basetable <- basetable %>%
  mutate(
    log_mean_donation = log1p(mean_donation_capped),
    log_total_donated = log1p(total_donated),
    log_tenure = log1p(tenure_days)
  )

# Interactions
basetable <- basetable %>%
  mutate(
    rfm_score = (1 / (days_since_last + 1)) * 
                donation_count * mean_donation,
    freq_recency = donation_count / (days_since_last + 1),
    trend_strength = abs(trend_percent) * total_donated
  )

# Dummies
library(fastDummies)
basetable <- dummy_cols(basetable,
                       select_columns = c("gender", "country"),
                       remove_first_dummy = TRUE,
                       remove_selected_columns = TRUE)
```

## Step 9: Train-Test Split
```{r eval=FALSE}
library(caret)

# Stratified split
set.seed(42)
train_index <- createDataPartition(
  basetable$target,
  p = 0.7,
  list = FALSE
)

train_data <- basetable[train_index, ]
test_data <- basetable[-train_index, ]

# Verify split
table(train_data$target) / nrow(train_data)
table(test_data$target) / nrow(test_data)
```

## Step 10: Feature Scaling
```{r eval=FALSE}
# Numeric features
numeric_features <- c(
  "days_since_last", "donation_count", "total_donated",
  "mean_donation_capped", "log_mean_donation", "rfm_score",
  "age", "tenure_days", "trend_percent"
)

# Calculate parameters
scaling_params <- train_data %>%
  summarize(across(all_of(numeric_features),
                  list(mean = ~mean(., na.rm = TRUE),
                       sd = ~sd(., na.rm = TRUE))))

saveRDS(scaling_params, "scaling_params.rds")

# Apply to train
train_scaled <- train_data
for (feat in numeric_features) {
  mean_col <- paste0(feat, "_mean")
  sd_col <- paste0(feat, "_sd")
  train_scaled[[feat]] <- (train_data[[feat]] - 
                          scaling_params[[mean_col]]) /
                          scaling_params[[sd_col]]
}

# Apply to test
test_scaled <- test_data
for (feat in numeric_features) {
  mean_col <- paste0(feat, "_mean")
  sd_col <- paste0(feat, "_sd")
  test_scaled[[feat]] <- (test_data[[feat]] - 
                         scaling_params[[mean_col]]) /
                         scaling_params[[sd_col]]
}
```

## Step 11: Model Training
```{r eval=FALSE}
library(glmnet)

# Prepare matrices
X_train <- model.matrix(target ~ . - donor_id - target_amount, 
                        data = train_scaled)[,-1]
y_train <- train_scaled$target

X_test <- model.matrix(target ~ . - donor_id - target_amount, 
                       data = test_scaled)[,-1]
y_test <- test_scaled$target

# Train LASSO
cv_model <- cv.glmnet(X_train, y_train,
                      family = "binomial",
                      alpha = 1,
                      nfolds = 5,
                      type.measure = "auc")

# Best lambda
best_lambda <- cv_model$lambda.min

# Final model
final_model <- glmnet(X_train, y_train,
                      family = "binomial",
                      alpha = 1,
                      lambda = best_lambda)
```

## Step 12: Evaluation
```{r eval=FALSE}
library(pROC)

# Predictions
test_predictions <- predict(final_model, 
                           newx = X_test, 
                           type = "response")[,1]

# ROC curve
roc_obj <- roc(y_test, test_predictions)
auc_value <- auc(roc_obj)

print(paste("Test AUC:", round(auc_value, 3)))

# Plot
plot(roc_obj, main = "ROC Curve",
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

# Confusion matrix
conf_matrix <- table(
  Predicted = as.integer(test_predictions > 0.5),
  Actual = y_test
)
print(conf_matrix)
```

## Step 13: Feature Importance
```{r eval=FALSE}
# Extract coefficients
coefficients <- coef(final_model)
coef_df <- data.frame(
  feature = rownames(coefficients),
  coefficient = as.vector(coefficients)
) %>%
  filter(coefficient != 0, feature != "(Intercept)") %>%
  arrange(desc(abs(coefficient)))

# Top 10
head(coef_df, 10)
```

## Step 14: Deployment
```{r eval=FALSE}
# Save artifacts
saveRDS(final_model, "donor_prediction_model.rds")
saveRDS(scaling_params, "scaling_parameters.rds")

# Scoring function
score_new_donors <- function(new_data, 
                             model_file = "donor_prediction_model.rds",
                             scaling_file = "scaling_parameters.rds") {
  # Load
  model <- readRDS(model_file)
  scaling <- readRDS(scaling_file)
  
  # Engineer features
  new_data <- engineer_features(new_data, Sys.Date())
  
  # Scale
  new_data_scaled <- scale_features(new_data, scaling)
  
  # Prepare matrix
  X_new <- model.matrix(~ . - donor_id, 
                       data = new_data_scaled)[,-1]
  
  # Score
  predictions <- predict(model, newx = X_new, 
                        type = "response")[,1]
  
  return(tibble(donor_id = new_data$donor_id, 
                prediction = predictions))
}
```

## Step 15: Monitoring
```{r eval=FALSE}
# Monitor predictions
monitor_predictions <- function(scored_data, baseline_stats) {
  current_mean <- mean(scored_data$prediction)
  current_sd <- sd(scored_data$prediction)
  
  alert <- FALSE
  
  # Check distribution shift
  if (abs(current_mean - baseline_stats$mean) > 0.1) {
    warning("Prediction mean shifted!")
    alert <- TRUE
  }
  
  if (abs(current_sd - baseline_stats$sd) / 
      baseline_stats$sd > 0.3) {
    warning("Prediction variance changed >30%!")
    alert <- TRUE
  }
  
  # Log
  log_entry <- tibble(
    timestamp = Sys.time(),
    mean_prediction = current_mean,
    sd_prediction = current_sd,
    alert_triggered = alert
  )
  
  write_csv(log_entry, "monitoring_log.csv", append = TRUE)
  
  return(log_entry)
}
```

# Summary

## Key Takeaways

\begin{block}{Remember}
\begin{enumerate}
  \item \textbf{Timeline integrity} is non-negotiable
  \item \textbf{RFM + Trends} capture 80\% of predictive power
  \item \textbf{Feature quality} > Feature quantity
  \item \textbf{Document everything}
  \item \textbf{Test systematically}
  \item \textbf{Monitor continuously}
\end{enumerate}
\end{block}

\begin{center}
\Large{\textcolor{datablue}{\textbf{Better features make better models!}}}
\end{center}

## Resources

**Books:**

- *Feature Engineering for Machine Learning* by Zheng & Casari
- *Feature Engineering and Selection* by Kuhn & Johnson
- *Data Science for Business* by Provost & Fawcett

**R Packages:**

- `recipes`: Production-ready feature engineering
- `caret`: Machine learning framework
- `tidyverse`: Data manipulation

## Next Lecture

**Topic:** Model Selection and Evaluation

- Logistic regression in depth
- Tree-based models
- Cross-validation strategies
- Performance metrics
- Model interpretation

\begin{center}
\Huge{\textcolor{datablue}{\textbf{Thank You!}}}

\Large{Questions?}
\end{center}